{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac16355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract gymfinder URLs from the crawled data\n",
    "def extract_gymfinder_urls(crawl_result):\n",
    "    \"\"\"\n",
    "    Extract gymfinder.ae URLs from FireCrawl result data\n",
    "    Returns a list of URLs without brackets\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    \n",
    "    # Check if we have data\n",
    "    if hasattr(crawl_result, 'data', pattern) and crawl_result.data:\n",
    "        for document in crawl_result.data:\n",
    "            if hasattr(document, 'markdown') and document.markdown:\n",
    "                found_urls = re.findall(pattern, document.markdown)\n",
    "                urls.extend(found_urls)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    unique_urls = list(dict.fromkeys(urls))\n",
    "    return unique_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c1e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from firecrawl import AsyncFirecrawlApp, ScrapeOptions\n",
    "\n",
    "async def crawl(url):\n",
    "    app = AsyncFirecrawlApp(api_key='fc-0f3f45b2c5ed43dcb40b13c439f7eae5')\n",
    "    \n",
    "    # For crawling multiple pages with limit\n",
    "    response = await app.crawl_url(\n",
    "        url=url,\n",
    "        limit=10,\n",
    "        scrape_options=ScrapeOptions(\n",
    "            formats=['markdown'],\n",
    "            only_main_content=True\n",
    "        )\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# For Jupyter notebooks, use await directly instead of asyncio.run()\n",
    "# Regex pattern to extract URLs from the crawled data\n",
    "# pattern = r'https://tax\\.gov\\.ae/en/tax\\.support/tax\\.agents/[^)]+/'\n",
    "# url = 'https://tax.gov.ae/en/tax.support/tax.agents/registered.tax.agents.aspx'\n",
    "# result = await crawl(url)\n",
    "\n",
    "# # Extract URLs from our crawl result\n",
    "# extracted_urls = extract_gymfinder_urls(result)\n",
    "\n",
    "# print(f\"Found {len(extracted_urls)} unique gym URLs:\")\n",
    "# for i, url in enumerate(extracted_urls, 1):\n",
    "#     print(f\"{i}. {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2276f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await crawl('https://tax.gov.ae/en/tax.support/tax.agents/registered.tax.agents.aspx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b7808",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in result.data:\n",
    "    print(data.markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63a0ea32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing crawl result and extracting member records...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def extract_member_records_from_markdown(markdown_text):\n",
    "    \"\"\"\n",
    "    Extract establishment member records from the markdown text\n",
    "    Returns a list of dictionaries containing member and establishment info\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    # Pattern to match TAAN sections (Tax Agent Registration Number)\n",
    "    taan_pattern = r'### TAAN: (\\d+)'\n",
    "    \n",
    "    # Split the markdown by TAAN sections\n",
    "    taan_matches = list(re.finditer(taan_pattern, markdown_text))\n",
    "    \n",
    "    for i, match in enumerate(taan_matches):\n",
    "        taan_number = match.group(1)\n",
    "        start_pos = match.start()\n",
    "        \n",
    "        # Find the end position (start of next TAAN or end of text)\n",
    "        if i + 1 < len(taan_matches):\n",
    "            end_pos = taan_matches[i + 1].start()\n",
    "        else:\n",
    "            end_pos = len(markdown_text)\n",
    "        \n",
    "        section = markdown_text[start_pos:end_pos]\n",
    "        \n",
    "        # Extract establishment name (first line after TAAN that's not empty)\n",
    "        lines = section.split('\\n')\n",
    "        establishment_name = \"\"\n",
    "        for line in lines[3:]:  # Skip TAAN line and empty lines\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#') and not line.startswith('-') and not line.startswith('['):\n",
    "                establishment_name = line\n",
    "                break\n",
    "        \n",
    "        # Extract location (usually comes after establishment name)\n",
    "        location = \"\"\n",
    "        found_name = False\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line == establishment_name:\n",
    "                found_name = True\n",
    "                continue\n",
    "            if found_name and line and not line.startswith('#') and not line.startswith('-') and not line.startswith('[') and not line.startswith('Previous Experience'):\n",
    "                location = line\n",
    "                break\n",
    "        \n",
    "        # Extract website\n",
    "        website_pattern = r'website\\.svg\\)([^\\\\n]+)'\n",
    "        website_match = re.search(website_pattern, section)\n",
    "        website = website_match.group(1) if website_match else \"\"\n",
    "        \n",
    "        # Extract emails\n",
    "        email_pattern = r'\\[([^@\\]]+@[^@\\]]+\\.[^@\\]]+)\\]'\n",
    "        emails = re.findall(email_pattern, section)\n",
    "        \n",
    "        # Extract phone numbers\n",
    "        phone_pattern = r'\\[(\\+\\d+)\\]'\n",
    "        phones = re.findall(phone_pattern, section)\n",
    "        \n",
    "        # Create records for each email/phone combination\n",
    "        if emails or phones:\n",
    "            # If we have both emails and phones, create combinations\n",
    "            if emails and phones:\n",
    "                for email in emails:\n",
    "                    for phone in phones:\n",
    "                        records.append({\n",
    "                            'taan_number': taan_number,\n",
    "                            'establishment_name': establishment_name,\n",
    "                            'location': location,\n",
    "                            'website': website,\n",
    "                            'email': email,\n",
    "                            'phone': phone,\n",
    "                            'extraction_timestamp': datetime.now().isoformat()\n",
    "                        })\n",
    "            # If only emails, create records for each email\n",
    "            elif emails:\n",
    "                for email in emails:\n",
    "                    records.append({\n",
    "                        'taan_number': taan_number,\n",
    "                        'establishment_name': establishment_name,\n",
    "                        'location': location,\n",
    "                        'website': website,\n",
    "                        'email': email,\n",
    "                        'phone': '',\n",
    "                        'extraction_timestamp': datetime.now().isoformat()\n",
    "                    })\n",
    "            # If only phones, create records for each phone\n",
    "            elif phones:\n",
    "                for phone in phones:\n",
    "                    records.append({\n",
    "                        'taan_number': taan_number,\n",
    "                        'establishment_name': establishment_name,\n",
    "                        'location': location,\n",
    "                        'website': website,\n",
    "                        'email': '',\n",
    "                        'phone': phone,\n",
    "                        'extraction_timestamp': datetime.now().isoformat()\n",
    "                    })\n",
    "        else:\n",
    "            # Create a record even if no contact info\n",
    "            records.append({\n",
    "                'taan_number': taan_number,\n",
    "                'establishment_name': establishment_name,\n",
    "                'location': location,\n",
    "                'website': website,\n",
    "                'email': '',\n",
    "                'phone': '',\n",
    "                'extraction_timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "    \n",
    "    return records\n",
    "\n",
    "def append_to_csv(records, filename='establishment_members.csv'):\n",
    "    \"\"\"\n",
    "    Append records to CSV file, creating it if it doesn't exist\n",
    "    \"\"\"\n",
    "    fieldnames = ['taan_number', 'establishment_name', 'location', 'website', 'email', 'phone', 'extraction_timestamp']\n",
    "    \n",
    "    file_exists = os.path.isfile(filename)\n",
    "    \n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        # Write header if file is new\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        # Write records\n",
    "        for record in records:\n",
    "            writer.writerow(record)\n",
    "    \n",
    "    print(f\"Appended {len(records)} records to {filename}\")\n",
    "    return filename\n",
    "\n",
    "def process_crawl_result(crawl_result, csv_filename='establishment_members.csv'):\n",
    "    \"\"\"\n",
    "    Process the crawl result and extract all member records to CSV\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    \n",
    "    if hasattr(crawl_result, 'data') and crawl_result.data:\n",
    "        for document in crawl_result.data:\n",
    "            if hasattr(document, 'markdown') and document.markdown:\n",
    "                records = extract_member_records_from_markdown(document.markdown)\n",
    "                all_records.extend(records)\n",
    "    \n",
    "    if all_records:\n",
    "        csv_file = append_to_csv(all_records, csv_filename)\n",
    "        print(f\"Total records extracted: {len(all_records)}\")\n",
    "        return csv_file, all_records\n",
    "    else:\n",
    "        print(\"No records found in the crawl result\")\n",
    "        return None, []\n",
    "\n",
    "# Test the extraction function with our result\n",
    "print(\"Processing crawl result and extracting member records...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a661941f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended 24 records to establishment_members.csv\n",
      "Total records extracted: 24\n",
      "\n",
      "Successfully extracted 24 records\n",
      "CSV file created/updated: establishment_members.csv\n",
      "\n",
      "First 3 records preview:\n",
      "\n",
      "Record 1:\n",
      "  taan_number: 20035861\n",
      "  establishment_name: Kmj Tax Consultant\n",
      "  location: Dubai\n",
      "  website: http://www.kmjtaxuae.com\n",
      "- ![](https://tax.gov.ae/e\n",
      "  email: taxagency.kmj@gmail.comm\n",
      "  phone: +971504200396\n",
      "  extraction_timestamp: 2025-08-19T15:42:30.222854\n",
      "\n",
      "Record 2:\n",
      "  taan_number: 20035861\n",
      "  establishment_name: Kmj Tax Consultant\n",
      "  location: Dubai\n",
      "  website: http://www.kmjtaxuae.com\n",
      "- ![](https://tax.gov.ae/e\n",
      "  email: olgishann@gmail.com\n",
      "  phone: +971504200396\n",
      "  extraction_timestamp: 2025-08-19T15:42:30.222863\n",
      "\n",
      "Record 3:\n",
      "  taan_number: 20013751\n",
      "  establishment_name: XB4 - DUBAI BRANCH\n",
      "  location: All Emirates\n",
      "  website: http://www.xb4.com\n",
      "- ![](https://tax.gov.ae/e\n",
      "  email: contact@xb4.com\n",
      "  phone: +971508161350\n",
      "  extraction_timestamp: 2025-08-19T15:42:30.222881\n",
      "\n",
      "Summary:\n",
      "- Total establishments: 12\n",
      "- Records with emails: 24\n",
      "- Records with phone numbers: 15\n"
     ]
    }
   ],
   "source": [
    "# Process the crawl result and create CSV\n",
    "csv_file, extracted_records = process_crawl_result(result, 'establishment_members.csv')\n",
    "\n",
    "if extracted_records:\n",
    "    print(f\"\\nSuccessfully extracted {len(extracted_records)} records\")\n",
    "    print(f\"CSV file created/updated: {csv_file}\")\n",
    "    \n",
    "    # Show first few records as preview\n",
    "    print(\"\\nFirst 3 records preview:\")\n",
    "    for i, record in enumerate(extracted_records[:3]):\n",
    "        print(f\"\\nRecord {i+1}:\")\n",
    "        for key, value in record.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Show summary statistics\n",
    "    total_establishments = len(set(record['establishment_name'] for record in extracted_records))\n",
    "    total_emails = len([r for r in extracted_records if r['email']])\n",
    "    total_phones = len([r for r in extracted_records if r['phone']])\n",
    "    \n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"- Total establishments: {total_establishments}\")\n",
    "    print(f\"- Records with emails: {total_emails}\")\n",
    "    print(f\"- Records with phone numbers: {total_phones}\")\n",
    "else:\n",
    "    print(\"No records were extracted!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb000672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for CSV management and incremental updates\n",
    "\n",
    "def view_csv_contents(filename='establishment_members.csv', max_rows=10):\n",
    "    \"\"\"View contents of the CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        print(f\"CSV file: {filename}\")\n",
    "        print(f\"Total rows: {len(df)}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        print(f\"\\nFirst {min(max_rows, len(df))} rows:\")\n",
    "        print(df.head(max_rows).to_string(index=False))\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"CSV file {filename} not found\")\n",
    "        return None\n",
    "\n",
    "def add_single_record(taan_number, establishment_name, location, website, email, phone, filename='establishment_members.csv'):\n",
    "    \"\"\"Add a single record to the CSV\"\"\"\n",
    "    record = {\n",
    "        'taan_number': taan_number,\n",
    "        'establishment_name': establishment_name,\n",
    "        'location': location,\n",
    "        'website': website,\n",
    "        'email': email,\n",
    "        'phone': phone,\n",
    "        'extraction_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    append_to_csv([record], filename)\n",
    "    print(f\"Added record for {establishment_name}\")\n",
    "    return record\n",
    "\n",
    "def remove_duplicates_from_csv(filename='establishment_members.csv'):\n",
    "    \"\"\"Remove duplicate records from CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        original_count = len(df)\n",
    "        \n",
    "        # Remove duplicates based on taan_number, email, and phone\n",
    "        df_unique = df.drop_duplicates(subset=['taan_number', 'email', 'phone'], keep='first')\n",
    "        \n",
    "        # Save back to CSV\n",
    "        df_unique.to_csv(filename, index=False)\n",
    "        \n",
    "        new_count = len(df_unique)\n",
    "        removed_count = original_count - new_count\n",
    "        \n",
    "        print(f\"Removed {removed_count} duplicate records\")\n",
    "        print(f\"File now has {new_count} unique records\")\n",
    "        \n",
    "        return df_unique\n",
    "    except FileNotFoundError:\n",
    "        print(f\"CSV file {filename} not found\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "# view_csv_contents()\n",
    "print(\"CSV management utilities loaded. Available functions:\")\n",
    "print(\"- view_csv_contents(): View the CSV file contents\")\n",
    "print(\"- add_single_record(): Add a manual record\")\n",
    "print(\"- remove_duplicates_from_csv(): Remove duplicate records\")\n",
    "print(\"- process_crawl_result(): Process new crawl results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the extraction and view results\n",
    "print(\"Testing extraction and viewing CSV contents...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# View the CSV contents\n",
    "df = view_csv_contents('establishment_members.csv', max_rows=5)\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"DETAILED ANALYSIS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Show establishments with multiple contacts\n",
    "    print(\"\\nEstablishments with multiple contact methods:\")\n",
    "    establishments = df.groupby('establishment_name').agg({\n",
    "        'email': lambda x: len([e for e in x if e]), \n",
    "        'phone': lambda x: len([p for p in x if p]),\n",
    "        'taan_number': 'first',\n",
    "        'location': 'first',\n",
    "        'website': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    multi_contact = establishments[(establishments['email'] > 1) | (establishments['phone'] > 1)]\n",
    "    if not multi_contact.empty:\n",
    "        print(multi_contact.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No establishments with multiple contacts found\")\n",
    "    \n",
    "    # Show unique locations\n",
    "    print(f\"\\nUnique locations found:\")\n",
    "    unique_locations = df['location'].unique()\n",
    "    for loc in unique_locations:\n",
    "        if loc:  # Only show non-empty locations\n",
    "            count = len(df[df['location'] == loc])\n",
    "            print(f\"  - {loc}: {count} records\")\n",
    "    \n",
    "    print(f\"\\nTotal unique emails: {len(df[df['email'] != '']['email'].unique())}\")\n",
    "    print(f\"Total unique phone numbers: {len(df[df['phone'] != '']['phone'].unique())}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo CSV file found. The extraction might not have run successfully.\")\n",
    "    print(\"Try running the previous cells first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f6da4",
   "metadata": {},
   "source": [
    "# Establishment Members CSV Extraction System\n",
    "\n",
    "## Overview\n",
    "This notebook extracts establishment member details from crawled tax agent data and exports them to a CSV file with incremental updates.\n",
    "\n",
    "## Features\n",
    "- **Automatic Data Extraction**: Parses markdown content to extract member records\n",
    "- **Multiple Contact Handling**: Creates separate records for each email/phone combination per establishment\n",
    "- **Incremental CSV Updates**: Appends new records without overwriting existing data\n",
    "- **Duplicate Management**: Tools to identify and remove duplicate entries\n",
    "- **Comprehensive Data Structure**: Includes TAAN number, establishment name, location, website, emails, and phone numbers\n",
    "\n",
    "## CSV Structure\n",
    "Each row in the CSV represents a member contact record with the following columns:\n",
    "- `taan_number`: Tax Agent Registration Number\n",
    "- `establishment_name`: Name of the establishment\n",
    "- `location`: Physical location/city\n",
    "- `website`: Official website URL\n",
    "- `email`: Contact email address\n",
    "- `phone`: Contact phone number\n",
    "- `extraction_timestamp`: When the record was extracted\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "### 1. Initial Extraction\n",
    "Run the crawling cells (1-3) to get the data, then run cell 5 to extract all records to CSV.\n",
    "\n",
    "### 2. Incremental Updates\n",
    "To process new crawl results and append to existing CSV:\n",
    "```python\n",
    "# After crawling new data\n",
    "new_csv_file, new_records = process_crawl_result(new_result, 'establishment_members.csv')\n",
    "```\n",
    "\n",
    "### 3. Manual Record Addition\n",
    "To add a single record manually:\n",
    "```python\n",
    "add_single_record(\n",
    "    taan_number=\"12345678\",\n",
    "    establishment_name=\"Example Company\",\n",
    "    location=\"Dubai\",\n",
    "    website=\"http://example.com\",\n",
    "    email=\"contact@example.com\",\n",
    "    phone=\"+971501234567\"\n",
    ")\n",
    "```\n",
    "\n",
    "### 4. CSV Management\n",
    "```python\n",
    "# View CSV contents\n",
    "view_csv_contents()\n",
    "\n",
    "# Remove duplicates\n",
    "remove_duplicates_from_csv()\n",
    "```\n",
    "\n",
    "## Data Handling Notes\n",
    "- **Multiple Contacts**: When an establishment has multiple emails or phone numbers, separate records are created for each combination\n",
    "- **Missing Data**: Records are created even when some contact information is missing\n",
    "- **Timestamps**: Each record includes when it was extracted for tracking purposes\n",
    "- **Encoding**: UTF-8 encoding ensures proper handling of special characters\n",
    "\n",
    "## File Output\n",
    "- **Default filename**: `establishment_members.csv`\n",
    "- **Location**: Same directory as this notebook\n",
    "- **Format**: Standard CSV with headers\n",
    "- **Append Mode**: New extractions are added to existing file without overwriting\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "growbal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
