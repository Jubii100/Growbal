{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f781b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from anthropic._exceptions import OverloadedError\n",
    "import os\n",
    "from pydantic import BaseModel, Field, model_validator\n",
    "from typing import List, Dict, Any\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pydantic import Field\n",
    "# from __future__ import print_function\n",
    "# from IPython.display import display\n",
    "from typing import Optional\n",
    "from utils import Country, ProviderType\n",
    "import anthropic\n",
    "# import traceback\n",
    "\n",
    "llm_model = os.environ[\"LLM_VERSION\"]\n",
    "\n",
    "# search_starters = [\"Digital Marketing Firm\", \"Tax Consultancy Firm\", \"Cybersecurity Firm\", \"Legal Services Provider\", \"Healthcare Consultancy\"]\n",
    "\n",
    "# search_starters = [\"Company formation service UAE\", \"Business formation service MENA\", \"Business registration services Gulf\", \"Business setup services Dubai\", \"Investment migration services UAE\", \"Second passport services\", \"Citizenship By investment\", \"economic passport services\", \"Golden Visa services\", \"Tax services UAE\", \"Tax Consultancy Firm Dubai\", \"Tax Calculation Services Dubai\"]\n",
    "\n",
    "class ResultRelevance(BaseModel):\n",
    "    explanation: str = Field(description=\"Be extremely brief and concise with your explanation.\")\n",
    "    link: str\n",
    "\n",
    "class RelevanceCheckOutput(BaseModel):\n",
    "    most_relevant_result: Optional[ResultRelevance]\n",
    "\n",
    "class SqlCommand(BaseModel):\n",
    "    command: str\n",
    "\n",
    "\n",
    "class ServiceProviderMemberDetails(BaseModel):\n",
    "    name: str = Field(description=\"Very important. Find the name of the staff member from the CLEAN HTML content.\")\n",
    "    role_description: str = Field(description=\"Find the role description of the staff member from the CLEAN HTML content.\")\n",
    "    telephone: str = Field(description=\"Find the telephone number of the staff member from the CLEAN HTML content.\")\n",
    "    mobile: str = Field(description=\"Find the mobile number of the staff member from the CLEAN HTML content.\")\n",
    "    email: str = Field(description=\"Find the email address of the staff member from the CLEAN HTML content.\")\n",
    "    linkedin: Optional[str]\n",
    "    facebook: Optional[str]\n",
    "    instagram: Optional[str]\n",
    "    twitter: Optional[str]\n",
    "    additional_info: Optional[str]\n",
    "\n",
    "class ServiceProviderOutput(BaseModel):\n",
    "    service_title: Optional[str] = Field(description=\"Produce a suitable title for the service.\")\n",
    "    service_description: Optional[str] = Field(description=\"Do your due diligence on summarising the service description based on a holistic consideration of the scattered relevant service info throughout the entirety of the given CLEAN HTML content. Use a string, not a number. Null if unavailable.\")\n",
    "    rating_score: Optional[float] = Field(description=\"Must adhere to a rating standard out of 5. Must be a numeric rating score of type float. Use a number, not a string. Null if unavailable.\")\n",
    "    rating_description: Optional[str] = Field(description=\"Do your due diligence on summarising the rating description based on a holistic consideration of the scattered relevant rating info throughout the entirety of the given CLEAN HTML content. Use a string, not a number. Null if unavailable.\")\n",
    "    pricing: Optional[str]\n",
    "    service_tags: List[str] = Field(description=\"Do your due diligence on summarising the service tags based on a holistic consideration of the scattered relevant service info throughout the entirety of the given CLEAN HTML content. Null if unavailable.\")\n",
    "    provider_type: ProviderType = Field(\"Company\", description=\"ProviderType enum. Should be Company if the provider is a company or Agent if the provider is an agent.\")\n",
    "    # provider_type: ProviderType = Field(description=\"ProviderType enum. Should be Company if the provider is a company or Agent if the provider is an agent.\")\n",
    "    country: Optional[Country] = Field(description=\"Country enum.\")\n",
    "    name: Optional[str] = Field(description=\"Very important. Find the name of the company from the CLEAN HTML content.\")\n",
    "    vision: Optional[str] = Field(description=\"A brief summary of the company/firm's vision statement from the CLEAN HTML content.\")\n",
    "    logo: Optional[str] = Field(description=\"Mandatory. Find the absolute URL for the company logo from the CLEAN HTML content.\")\n",
    "    website: Optional[str] = Field(description=\"Very important. Must find the website link of the company from the HTML content or infer it from the base website link address. Null if unavailable.\")\n",
    "    linkedin: Optional[str] = Field(description=\"Null if unavailable.\")\n",
    "    facebook: Optional[str] = Field(description=\"Null if unavailable.\")\n",
    "    instagram: Optional[str] = Field(description=\"Null if unavailable.\")\n",
    "    telephones: List[str] = Field([], description=\"Mandatory. Must find at least one telephone number. Field must be a list of telephone numbers or empty list.\")\n",
    "    mobiles: List[str] = Field([], description=\"Mandatory. Must find at least one mobile number. Field must be a list of mobile numbers or empty list.\")\n",
    "    emails: List[str] = Field(\n",
    "        [], description=\"Mandatory. Must find at least one email address of the company from the HTML content. Field must be a list of email addresses or empty list.\"\n",
    "    )\n",
    "    # emails: List[str] = Field(description=\"Very very important. Must find at least one email address of the company from the HTML content. Output must be a list of email addresses.\")\n",
    "    office_locations: Optional[str] = Field(description=\"Detailed address locations of the company offices eg. 123 Main St, City, Country, or description if address is not available. Null if unavailable.\")\n",
    "    key_individuals: Optional[str] = Field(description=\"Names and role descriptions of key people of the company. Null if unavailable.\")\n",
    "    representatives: Optional[str] = Field(description=\"Names and roles and contact details of representatives of the company. Null if unavailable.\")\n",
    "    # service_provider_member_details: Optional[List[ServiceProviderMemberDetails]]\n",
    "\n",
    "class SearchTermOutput(BaseModel):\n",
    "    search_term: str\n",
    "\n",
    "class AboutUsOutput(BaseModel):\n",
    "    about_us_link: Optional[List[str]]\n",
    "\n",
    "def call_llm(model = \"claude-3-5-sonnet-20241022\", API_key = os.environ[\"ANTHROPIC_API_KEY\"], prompt_template = None, input_dict = None, structured_output = None, temperature = 0):\n",
    "    if prompt_template is None:\n",
    "        raise ValueError(\"Prompt template is required\")\n",
    "    if input_dict is None:\n",
    "        raise ValueError(\"Input dictionary is required\")\n",
    "    if structured_output is None:\n",
    "        raise ValueError(\"Structured output is required\")\n",
    "    llm = ChatAnthropic(\n",
    "        model=model,\n",
    "        anthropic_api_key=API_key, temperature=temperature).with_structured_output(structured_output)\n",
    "    llm_chain = prompt_template | llm\n",
    "    llm_chain = llm_chain.with_retry(\n",
    "        retry_if_exception_type=(OverloadedError,),\n",
    "        wait_exponential_jitter=True,    \n",
    "        stop_after_attempt=6\n",
    "    )\n",
    "    result = llm_chain.invoke(input_dict)\n",
    "    return result\n",
    "\n",
    "def search_serper(search_query):\n",
    "    url = \"https://google.serper.dev/search\"\n",
    "    \n",
    "    payload = json.dumps({\n",
    "        \"q\": search_query,\n",
    "        \"gl\": \"ae\", \n",
    "        \"num\": 10,\n",
    "        # \"tbs\": \"qdr:w\"\n",
    "    })\n",
    "\n",
    "    headers = {\n",
    "        'X-API-KEY': os.environ[\"SERPER_API_KEY\"],\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload, timeout=(3,4))\n",
    "    results = json.loads(response.text)\n",
    "    results_list = results['organic']\n",
    "\n",
    "    all_results = []\n",
    "    for id, result in enumerate(results_list, 1):\n",
    "        result_dict = {\n",
    "            'title': result['title'],\n",
    "            'link': result['link'],\n",
    "            'snippet': result['snippet'],\n",
    "            'search_term': search_query,\n",
    "            'id': id\n",
    "        }\n",
    "        all_results.append(result_dict)\n",
    "    return all_results\n",
    "\n",
    "def load_prompt(prompt_name):\n",
    "    with open(f\"prompts/{prompt_name}.md\", \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "\n",
    "def check_search_relevance(search_term: str, search_results: Dict[str, Any]) -> RelevanceCheckOutput:\n",
    "    prompt = load_prompt(\"relevance_check_one_link\")\n",
    "    \n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", prompt),\n",
    "        (\"human\", f\"Check relevance of search results to the given search term (but most importantly make sure it is the actual company's official website, not a search facilitator website or other service of that sort): {search_term}\")\n",
    "    ])\n",
    "    \n",
    "    return call_llm(prompt_template = prompt_template, input_dict = {\"search_term\": search_term, 'search_results': search_results}, structured_output = RelevanceCheckOutput, temperature=0)\n",
    "\n",
    "\n",
    "def convert_html_to_markdown(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Headers\n",
    "    for h in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "        level = int(h.name[1])\n",
    "        h.replace_with('#' * level + ' ' + h.get_text() + '\\n\\n')\n",
    "    \n",
    "    # Links\n",
    "    for a in soup.find_all('a'):\n",
    "        href = a.get('href', '')\n",
    "        text = a.get_text()\n",
    "        if href and text:\n",
    "            a.replace_with(f'[{text}]({href})')\n",
    "    \n",
    "    # Bold\n",
    "    for b in soup.find_all(['b', 'strong']):\n",
    "        b.replace_with(f'**{b.get_text()}**')\n",
    "    \n",
    "    # Italic\n",
    "    for i in soup.find_all(['i', 'em']):\n",
    "        i.replace_with(f'*{i.get_text()}*')\n",
    "    \n",
    "    # Lists\n",
    "    for ul in soup.find_all('ul'):\n",
    "        for li in ul.find_all('li'):\n",
    "            li.replace_with(f'- {li.get_text()}\\n')\n",
    "    \n",
    "    for ol in soup.find_all('ol'):\n",
    "        for i, li in enumerate(ol.find_all('li'), 1):\n",
    "            li.replace_with(f'{i}. {li.get_text()}\\n')\n",
    "    \n",
    "    # Get text and clean up\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Remove excess whitespace/newlines\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def scrape_and_save_markdown(relevant_results):\n",
    "    markdown_contents = []\n",
    "    for result in relevant_results:\n",
    "        if 'link' in result:\n",
    "            payload = {\n",
    "                \"api_key\": os.environ[\"SCRAPING_API_KEY\"], \n",
    "                \"url\": result['link'],\n",
    "                \"render_js\": \"true\"\n",
    "            }\n",
    "\n",
    "            response = requests.get(\"https://scraping.narf.ai/api/v1/\", params=payload)\n",
    "            if response.status_code == 200:\n",
    "                markdown_content = convert_html_to_markdown(response.content.decode())\n",
    "                \n",
    "                markdown_contents.append({\n",
    "                    'url': result['link'],\n",
    "                    'markdown': markdown_content,\n",
    "                    'title': result.get('title', ''),\n",
    "                    'id': result.get('id', '')\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Failed to fetch {result['link']}: Status code {response.status_code}\")\n",
    "\n",
    "    print(f\"Successfully downloaded and saved {len(markdown_contents)} pages as markdown\")\n",
    "    return markdown_contents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scrapper import HtmlScraper\n",
    "import tiktoken\n",
    "import string\n",
    "import secrets\n",
    "\n",
    "import os\n",
    "import django\n",
    "import sys\n",
    "from asgiref.sync import sync_to_async\n",
    "from urllib.parse import urlparse\n",
    "from django.core.validators import validate_email, URLValidator\n",
    "from django.core.exceptions import ValidationError\n",
    "\n",
    "sys.path.insert(0, '../growbal_django')\n",
    "\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"growbal.settings\")\n",
    "\n",
    "django.setup()\n",
    "\n",
    "from services.models import Service\n",
    "from services.serializers import ServiceSerializer\n",
    "from accounts.serializers import ServiceProviderProfileSerializer\n",
    "from accounts.models import CustomUser, ServiceProviderProfile\n",
    "from asgiref.sync import sync_to_async\n",
    "# from django.db import transaction\n",
    "from scraper.models import Scrape          # adjust import path as needed\n",
    "from scraper.serializers import ScrapeSerializer\n",
    "# from django.core.files import File\n",
    "\n",
    "def validate_emails(email_list):\n",
    "    valid_emails = []\n",
    "    for email in email_list:\n",
    "        try:\n",
    "            validate_email(email)\n",
    "            valid_emails.append(email)\n",
    "        except ValidationError:\n",
    "            error_message = f\"Error validating email {email}\"\n",
    "            print(error_message)\n",
    "            with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                log_file.write(error_message + \"\\n\")\n",
    "    return valid_emails\n",
    "\n",
    "\n",
    "_url_validator = URLValidator()\n",
    "def validate_url(raw_url: str | None) -> bool:\n",
    "    if not raw_url:\n",
    "        return False\n",
    "\n",
    "    raw_url = raw_url.strip()\n",
    "    if not urlparse(raw_url).scheme:\n",
    "        raw_url = \"http://\" + raw_url\n",
    "\n",
    "    try:\n",
    "        _url_validator(raw_url)\n",
    "        return True\n",
    "    except ValidationError:\n",
    "        return False\n",
    "\n",
    "\n",
    "@sync_to_async\n",
    "def get_or_create_user(name=None, email=None, username=None, password=None):\n",
    "    user, created = CustomUser.objects.get_or_create_user(\n",
    "        name=name,\n",
    "        email=email,\n",
    "        username=username,\n",
    "        password=password\n",
    "    )\n",
    "    return user, created\n",
    "\n",
    "@sync_to_async\n",
    "def create_service(data):\n",
    "    serializer = ServiceSerializer(data=data)\n",
    "    if serializer.is_valid():\n",
    "        service = serializer.save()\n",
    "        return service\n",
    "    else:\n",
    "        error_message = f\"Error validating service data {serializer.errors}\"\n",
    "        print(error_message)\n",
    "        with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "            log_file.write(error_message + \"\\n\")\n",
    "\n",
    "@sync_to_async\n",
    "def create_service_provider_profile(data):\n",
    "    serializer = ServiceProviderProfileSerializer(data=data)\n",
    "    if serializer.is_valid():\n",
    "        profile = serializer.save()\n",
    "        return profile\n",
    "    else:\n",
    "        error_message = f\"Error validating service provider profile data {serializer.errors}\"\n",
    "        print(error_message)\n",
    "        with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "            log_file.write(error_message + \"\\n\")\n",
    "        # raise ValueError(serializer.errors)\n",
    "\n",
    "@sync_to_async\n",
    "def create_scrape(data):\n",
    "    serializer = ScrapeSerializer(data=data)\n",
    "    if serializer.is_valid():\n",
    "        scrape = serializer.save()\n",
    "        return scrape\n",
    "    else:\n",
    "        error_message = f\"Error validating scrape data {serializer.errors}\"\n",
    "        print(error_message)\n",
    "        with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "            log_file.write(error_message + \"\\n\")\n",
    "        # raise ValueError(serializer.errors)\n",
    "\n",
    "@sync_to_async\n",
    "def check_similar_scrapes(base_url):\n",
    "    return Scrape.check_similar_base_url(base_url=base_url)\n",
    "\n",
    "def generate_password(length=8):\n",
    "    characters = string.ascii_letters + string.digits + string.punctuation\n",
    "    password = ''.join(secrets.choice(characters) for _ in range(length))\n",
    "    return password\n",
    "\n",
    "# def count_tokens(text, model=\"gpt-4\"):\n",
    "#     encoding = tiktoken.encoding_for_model(model)\n",
    "#     tokens = encoding.encode(text)\n",
    "#     return len(tokens)\n",
    "\n",
    "anthropic_client = anthropic.Anthropic()\n",
    "\n",
    "def count_tokens(text, model=\"claude-3-5-sonnet-20241022\", system_prompt=\"\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": text}]\n",
    "    \n",
    "    response = anthropic_client.messages.count_tokens(\n",
    "        model=model,\n",
    "        system=system_prompt,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.input_tokens\n",
    "\n",
    "scraper = HtmlScraper(scraping_api_key=os.environ[\"SCRAPING_API_KEY\"])\n",
    "\n",
    "async def process_relevant_links(relevant_link, row_dict):\n",
    "    if await check_similar_scrapes(relevant_link.link):\n",
    "        return\n",
    "    else:\n",
    "        nav_html_list = []\n",
    "        try:\n",
    "                num_tokens_allowed = 38000\n",
    "                max_page_tokens = 13000\n",
    "                # min_page_num = 7\n",
    "                i = 1\n",
    "                clean_html_sum_links = []\n",
    "                clean_html_sum = \"\"\n",
    "                try:\n",
    "                    nav_html_content_about = scraper.scrape_html_about(relevant_link.link)\n",
    "                    if nav_html_content_about:\n",
    "                        nav_html_list.append(nav_html_content_about)\n",
    "                except Exception as e:\n",
    "                    error_message = f\"Error processing about page: {e}\"\n",
    "                    print(error_message)\n",
    "                    with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                        log_file.write(error_message + \"\\n\")\n",
    "\n",
    "                try:\n",
    "                    nav_html_content_contact = scraper.scrape_html_contact(relevant_link.link)\n",
    "                    if nav_html_content_contact:\n",
    "                        nav_html_list.append(nav_html_content_contact)\n",
    "                except Exception as e:\n",
    "                    error_message = f\"Error processing contact page: {e}\"\n",
    "                    print(error_message)\n",
    "                    with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                        log_file.write(error_message + \"\\n\")\n",
    "\n",
    "                try:\n",
    "                    print(\"----------BASE PAGE----------\")\n",
    "                    print(relevant_link.link)\n",
    "                    nav_html_content_base = scraper.scrape_html_base(relevant_link.link)\n",
    "                    \n",
    "                    clean_text, clean_html = scraper.clean_html_content(nav_html_content_base)\n",
    "                    # scraper.save_html(clean_html, relevant_link.link)\n",
    "                    if clean_html and (token_count := count_tokens(clean_html)) < max_page_tokens/2:\n",
    "                        print(f\"clean_html: {token_count} tokens\")\n",
    "                        print(\"approved\")\n",
    "                        clean_html_sum += f\"CLEAN HTML PAGE {i}: ({relevant_link.link})\\n\\n\"\n",
    "                        # use slicing pipeline to get the slices from the clean html\n",
    "                        clean_html_sum += clean_html + \"\\n\\n\"\n",
    "                        clean_html_sum_links.append(relevant_link.link)\n",
    "                    elif clean_text and (token_count := count_tokens(clean_text)) < max_page_tokens:\n",
    "                        print(f\"clean_text: {token_count} tokens\")\n",
    "                        print(\"approved\")\n",
    "                        clean_html_sum += f\"ONLY TEXT FROM HTML PAGE {i}: ({relevant_link.link})\\n\\n\"\n",
    "                        clean_html_sum += clean_text + \"\\n\\n\"\n",
    "                        clean_html_sum_links.append(relevant_link.link)\n",
    "                    if nav_html_content_base:\n",
    "                        nav_html_list.append(nav_html_content_base)\n",
    "                except Exception as e:\n",
    "                    error_message = f\"Error processing base page: {e}\"\n",
    "                    print(error_message)\n",
    "                    with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                        log_file.write(error_message + \"\\n\")\n",
    "\n",
    "                nav_links = []\n",
    "                if len(nav_html_list) == 0 or not nav_html_list[0]: \n",
    "                    return\n",
    "                for html_content in nav_html_list:\n",
    "                    try:\n",
    "                        nav_links.extend(scraper.get_nav_links(html_content))\n",
    "                    except Exception as e:\n",
    "                        error_message = f\"Error processing nav links: {e}\"\n",
    "                        print(error_message)\n",
    "                        with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                            log_file.write(error_message + \"\\n\")\n",
    "                print(\"----------NAV LINKS----------\")\n",
    "                print(nav_links)\n",
    "                nav_links_str = \"\\n\".join([f\"{title}: {link}\" for title, link, _ in nav_links])\n",
    "\n",
    "                prompt = load_prompt(\"generate_about_us_link\")\n",
    "                prompt_template = ChatPromptTemplate.from_messages([\n",
    "                    (\"system\", prompt),\n",
    "                    (\"human\", f'Filter \"about us\" page, contact us page, services page, team page, or any similar page.')\n",
    "                ])\n",
    "                \n",
    "\n",
    "                result = call_llm(prompt_template = prompt_template, input_dict = {\"nav_links\": nav_links_str, \"base_url\": relevant_link.link}, structured_output = AboutUsOutput, temperature=0)\n",
    "                print(len(result.about_us_link))\n",
    "                print(result.about_us_link)\n",
    "                print(\"\\n\")\n",
    "                for i, link in enumerate(result.about_us_link):\n",
    "                    if link in clean_html_sum_links:\n",
    "                        continue\n",
    "                    if clean_html_sum and count_tokens(clean_html_sum) >= num_tokens_allowed:\n",
    "                        continue\n",
    "                    try:\n",
    "                        html_content = scraper.scrape_html_base(link)\n",
    "                        clean_text, clean_html = scraper.clean_html_content(html_content)\n",
    "                        \n",
    "                        if clean_html: print(f\"count_tokens(clean_html) {i} before approval: {count_tokens(clean_html)}\")\n",
    "                        else: print(f\"not clean_text {i} before approval: {clean_html}\")\n",
    "\n",
    "                        if clean_html and (count_tokens(clean_html) < max_page_tokens and count_tokens(clean_html) < num_tokens_allowed - count_tokens(clean_html_sum) if clean_html_sum else 0):\n",
    "                            clean_html_sum += f\"CLEAN HTML PAGE {i}: ({link})\\n\\n\"\n",
    "                            # use slicing pipeline to get the slices from the clean html\n",
    "                            clean_html_sum += clean_html + \"\\n\\n\"\n",
    "                            clean_html_sum_links.append(link)\n",
    "                            print(f\"{i} approved {link}\")\n",
    "                        elif (token_count := count_tokens(clean_html_sum)) < num_tokens_allowed:\n",
    "                            print(token_count)\n",
    "                            clean_html_sum += f\"ONLY TEXT FROM HTML PAGE {i}: ({link})\\n\\n\"\n",
    "                            clean_html_sum += clean_text + \"\\n\\n\"\n",
    "                            clean_html_sum_links.append(link)\n",
    "                            print(f\"{i} approved {link}\")\n",
    "                        else:\n",
    "                            break\n",
    "                        i += 1\n",
    "                    except Exception as e:\n",
    "                        error_message = f\"Error processing {link}: {e}\"\n",
    "                        print(error_message)\n",
    "                        with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                            log_file.write(error_message + \"\\n\")\n",
    "                \n",
    "                # print(\"##########PROPRIETARY DATA RECORD OF THE ORGANIZATION##########\\n\")\n",
    "                # print(str(row_dict))\n",
    "                # with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                #     log_file.write(\"PROPRIETARY DATA RECORD OF THE ORGANIZATION\\n\\n\" + str(row_dict) + \"\\n\\n\")\n",
    "                # html_appendex = \"PROPRIETARY DATA RECORD OF THE ORGANIZATION\\n\\n\" + str(row_dict)\n",
    "                clean_html_sum = clean_html_sum[:int((num_tokens_allowed*4)/2)]\n",
    "                if clean_html_sum != \"\":\n",
    "                    prompt = load_prompt(\"generate_fields_with_email\")\n",
    "                    prompt_template = ChatPromptTemplate.from_messages([\n",
    "                        (\"system\", prompt),\n",
    "                        (\"human\", f\"Generate fields from the given HTML content.\")\n",
    "                    ])\n",
    "                    \n",
    "                    result = call_llm(prompt_template = prompt_template, input_dict = {\"html_content\": clean_html_sum, \"proprietary_data\": str(row_dict)}, structured_output = ServiceProviderOutput, temperature=0)\n",
    "                    if result.emails == None:\n",
    "                        print(\"result.email is Null\")\n",
    "                        result.emails = []\n",
    "                    if result.telephones == None:\n",
    "                        print(\"result.telephones is Null\")\n",
    "                        result.telephones = []\n",
    "                    if result.mobiles == None:\n",
    "                        print(\"result.mobiles is Null\")\n",
    "                        result.mobiles = []\n",
    "                    if result.logo:\n",
    "                        try:\n",
    "                            result.logo = scraper.save_logo(result.logo)\n",
    "                        except Exception as e:\n",
    "                            error_message = f\"Error saving logo: {e}\"\n",
    "                            print(error_message)\n",
    "                            with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                                log_file.write(error_message + \"\\n\")\n",
    "                            result.logo = None\n",
    "\n",
    "                result.emails = validate_emails(result.emails)\n",
    "                if hasattr(result, 'name') and result.name and result.name != \"\":\n",
    "                    if hasattr(result, 'name') and result.name and result.name != \"\":\n",
    "                        user = None\n",
    "                        if validate_url(result.website):\n",
    "                            result.website = result.website.strip()\n",
    "                            if not urlparse(result.website).scheme:\n",
    "                                result.website = \"http://\" + result.website\n",
    "                            try:\n",
    "                                profile = await ServiceProviderProfile.objects.aget(website=result.website)\n",
    "                                if profile:\n",
    "                                    log_message = f\"Profile already exists for this website: {result.website}\"\n",
    "                                    print(log_message)\n",
    "                                    with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                                        log_file.write(log_message + \"\\n\")\n",
    "                                    user = profile.user\n",
    "                            except Exception as e:\n",
    "                                profile = None\n",
    "                                error_message = f\"Error getting profile from website: {e}\"\n",
    "                                print(error_message)\n",
    "                                with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                                    log_file.write(error_message + \"\\n\")\n",
    "                        else:\n",
    "                            result.website = None\n",
    "                        \n",
    "                        if not user and hasattr(result, 'emails') and result.emails and result.emails[0] != \"\":\n",
    "                            print(\"creating user with name and email\")\n",
    "                            user_email = result.emails[0]\n",
    "                            user_password = generate_password(8)\n",
    "                            user, created = await get_or_create_user(name=result.name, email=user_email, username=user_email, password=user_password)\n",
    "                            print(\"created user with email and name\")\n",
    "                        elif not user:\n",
    "                            user_password = generate_password(8)\n",
    "                            user, created = await get_or_create_user(name=result.name, password=user_password)\n",
    "                            print(\"created user with name only\")                        \n",
    "\n",
    "                        if user:\n",
    "                            # if not result.emails: result.emails = []\n",
    "                            try:\n",
    "                                profile = await ServiceProviderProfile.objects.aget(user_id=user.id)\n",
    "                            except ServiceProviderProfile.DoesNotExist:\n",
    "                                profile = None\n",
    "                                error_message = f\"Error profile does not exist!\"\n",
    "                                print(error_message)\n",
    "                                with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                                    log_file.write(error_message + \"\\n\")\n",
    "                            except Exception as e:\n",
    "                                profile = None\n",
    "                                error_message = f\"Error getting profile: {e}\"\n",
    "                                print(error_message)\n",
    "                                with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                                    log_file.write(error_message + \"\\n\")\n",
    "                            \n",
    "                            if not created and profile:\n",
    "                                data = {\n",
    "                                    \"provider_type\": result.provider_type[:50] if result.provider_type else profile.provider_type,\n",
    "                                    \"country\": result.country[:100] if result.country else profile.country,\n",
    "                                    \"session_status\": profile.session_status,\n",
    "                                    \"name\": result.name[:255] if result.name else profile.name,\n",
    "                                    \"vision\": result.vision or profile.vision,\n",
    "                                    \"website\": result.website if validate_url(result.website) else profile.website,\n",
    "                                    \"logo\": result.logo or profile.logo,\n",
    "                                    \"linkedin\": result.linkedin if validate_url(result.linkedin) else profile.linkedin,\n",
    "                                    \"facebook\": result.facebook if validate_url(result.facebook) else profile.facebook,\n",
    "                                    \"instagram\": result.instagram if validate_url(result.instagram) else profile.instagram,\n",
    "                                    \"telephones\": [x[:30] for x in result.telephones] if result.telephones else profile.telephones,\n",
    "                                    \"mobiles\": [x[:30] for x in result.mobiles] if result.mobiles else profile.mobiles,\n",
    "                                    \"emails\": result.emails if result.emails else profile.emails,\n",
    "                                    \"office_locations\": result.office_locations or profile.office_locations,\n",
    "                                    \"key_individuals\": result.key_individuals or profile.key_individuals,\n",
    "                                    \"representatives\": result.representatives or profile.representatives\n",
    "                                }\n",
    "                                saved_profile, created_profile = await ServiceProviderProfile.objects.aupdate_or_create(\n",
    "                                    user_id=user.id,             # lookup part – must stay unique\n",
    "                                    defaults=data            # values to update/insert\n",
    "                                )   \n",
    "                                print(f\"Profile updated: {saved_profile.name}\")\n",
    "                            else:\n",
    "                                data = {\n",
    "                                    \"user\": user.username,\n",
    "                                    \"provider_type\": result.provider_type[:50] if result.provider_type else None,\n",
    "                                    \"country\": result.country[:100] if result.country else None,\n",
    "                                    \"session_status\": \"inactive\",\n",
    "                                    \"name\": result.name[:255] if result.name else None,\n",
    "                                    \"vision\": result.vision,\n",
    "                                    \"website\": result.website if validate_url(result.website) else None,\n",
    "                                    \"logo\": result.logo,\n",
    "                                    \"linkedin\": result.linkedin if validate_url(result.linkedin) else None,\n",
    "                                    \"facebook\": result.facebook if validate_url(result.facebook) else None,\n",
    "                                    \"instagram\": result.instagram if validate_url(result.instagram) else None,\n",
    "                                    \"telephones\": [x[:30] for x in result.telephones] if result.telephones else [],\n",
    "                                    \"mobiles\": [x[:30] for x in result.mobiles] if result.mobiles else [],\n",
    "                                    \"emails\": result.emails,\n",
    "                                    \"office_locations\": result.office_locations,\n",
    "                                    \"key_individuals\": result.key_individuals,\n",
    "                                    \"representatives\": result.representatives\n",
    "                                }\n",
    "                                saved_profile = await create_service_provider_profile(data)\n",
    "                                print(f\"Profile created: {saved_profile.name}\")\n",
    "\n",
    "                            service_data_json = {\n",
    "                                \"profile\": saved_profile.id,\n",
    "                                \"service_title\": result.service_title[:255] if result.service_title else None,\n",
    "                                \"service_description\": result.service_description,\n",
    "                                \"service_tags\": result.service_tags,\n",
    "                                \"rating_score\": result.rating_score,\n",
    "                                \"rating_description\": result.rating_description,\n",
    "                                \"pricing\": result.pricing\n",
    "                            }\n",
    "                            # Direct async call (works in Jupyter/IPython):\n",
    "                            saved_service = await create_service(service_data_json)\n",
    "                            print(f\"Service created: {saved_service.service_title}\")\n",
    "                            scrape_data = {\n",
    "                                \"base_url\": relevant_link.link,\n",
    "                                \"provider\": saved_profile.id,\n",
    "                                \"service\": saved_service.id,\n",
    "                                \"cleaned_html\": clean_html_sum\n",
    "                            }\n",
    "                            saved_scrape = await create_scrape(scrape_data)\n",
    "                            print(f\"Scrape created: {saved_scrape.base_url}\")\n",
    "        except Exception as e:\n",
    "            # raise ValueError(e)\n",
    "            error_message = f\"Error processing {relevant_link.link}: {e}\"\n",
    "            print(error_message)\n",
    "            with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                log_file.write(error_message + \"\\n\")\n",
    "                # traceback.print_exc(file=log_file)\n",
    "\n",
    "import argparse\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "async def main(env_path: str) -> None:\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"Loaded envs from {env_path}\")\n",
    "\n",
    "    with open(os.environ[\"LOG_FILE_PATH\"], \"w\") as log_file:\n",
    "        log_file.write(\"\")\n",
    "\n",
    "    missed_df = pd.DataFrame()\n",
    "    \n",
    "    # Create or overwrite the missed entries file with an empty CSV\n",
    "    missed_df_path = os.environ[\"MISSED_ENTRIES_PATH\"]\n",
    "    missed_df.to_csv(missed_df_path, index=False)\n",
    "    \n",
    "    df = pd.read_csv(os.environ[\"EMAIL_LIST_PATH\"])\n",
    "    df = df[['ORGANIZATION','INDUSTRY', 'WEBSITE', 'COUNTRY', 'CITY', 'FIRSTNAME', 'LASTNAME', 'EMAIL', 'LINKEDIN', 'DESIGNATION']]\n",
    "    df['ORGANIZATION_COPY'] = df['ORGANIZATION']\n",
    "    df = df.set_index('ORGANIZATION_COPY')\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    grouped = (\n",
    "        df.groupby(df.index)\n",
    "        .agg(lambda x: x.dropna().iloc[0] if x.notna().any() else None)\n",
    "    )\n",
    "    i = 1\n",
    "    successful_entries = 0\n",
    "    for organization, group in grouped.iterrows():\n",
    "        log_message = f\"Processing Entry {i}/{len(grouped)} Organization: {organization}\"\n",
    "        print(log_message)\n",
    "        with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "            log_file.write(log_message + \"\\n\")\n",
    "        row_dict = group.to_dict()\n",
    "        prompt = load_prompt(\"generate_a_search_term\")\n",
    "        prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", prompt),\n",
    "            (\"human\", f\"Generate only one suitable google search term to fetch the official website of the company/service provider/orginization: {row_dict['ORGANIZATION']}\")\n",
    "        ])\n",
    "\n",
    "        result = call_llm(prompt_template = prompt_template, input_dict = {\"row_dict\": row_dict}, structured_output = SearchTermOutput, temperature=0)\n",
    "\n",
    "        try:\n",
    "            print(f\"Processing search term: {result.search_term}\")\n",
    "            links = search_serper(result.search_term)\n",
    "            # print(f\"######### Found {len(links)} links ###########\")\n",
    "            # print(links)\n",
    "            most_relevant_result = check_search_relevance(result.search_term, links).most_relevant_result\n",
    "            if most_relevant_result:\n",
    "                print(most_relevant_result)\n",
    "                await process_relevant_links(most_relevant_result, row_dict)\n",
    "                successful_entries += 1\n",
    "                log_message = f\"Successful processed entries {successful_entries}/{len(grouped)} Organization: {organization}\"\n",
    "                print(log_message)\n",
    "                with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                    log_file.write(log_message + \"\\n\")\n",
    "            else:\n",
    "                missed_df = pd.concat([missed_df, group.to_frame().T], ignore_index=True)\n",
    "                missed_df_path = os.environ[\"MISSED_ENTRIES_PATH\"]\n",
    "                missed_df.to_csv(missed_df_path, index=False)\n",
    "                log_message = f\"No relevant link found for search term: {result.search_term}\"\n",
    "                print(log_message)\n",
    "                with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                    log_file.write(log_message + \"\\n\")\n",
    "        except Exception as e:\n",
    "            missed_df = pd.concat([missed_df, group.to_frame().T], ignore_index=True)\n",
    "            missed_df_path = os.environ[\"MISSED_ENTRIES_PATH\"]\n",
    "            missed_df.to_csv(missed_df_path, index=False)\n",
    "            error_message = f\"Error processing Entry {i}/{len(grouped)} Organization: {organization}: {e}\"\n",
    "            print(error_message)\n",
    "            with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                log_file.write(error_message + \"\\n\")\n",
    "        i += 1\n",
    "        # if i == 10:\n",
    "        #     break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Run the crawl with an .env file.\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"env_path\",\n",
    "        help=\"Path to the .env file (e.g., ../envs/1.env).\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    asyncio.run(main(args.env_path))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
