{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fixed HTML Slicing Pipeline\n",
        "\n",
        "This notebook implements a working HTML slicing pipeline that:\n",
        "- Uses LangChain and the existing `generate_slices.md` prompt\n",
        "- Passes Pydantic model schema as input to avoid template conflicts\n",
        "- Uses direct LLM calls to avoid ChatPromptTemplate + HTML issues\n",
        "- Maintains compatibility with existing code structure\n",
        "\n",
        "## Key Fixes Applied:\n",
        "1. **Direct LLM invocation** instead of ChatPromptTemplate with HTML\n",
        "2. **Schema injection** into prompt to maintain structure\n",
        "3. **Robust JSON parsing** with fallback handling\n",
        "4. **Conservative changes** to existing codebase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from anthropic._exceptions import OverloadedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Existing Pydantic models (unchanged)\n",
        "class Slice(BaseModel):\n",
        "    \"\"\"A contiguous, inclusive range of 0-based line numbers that contains\n",
        "    content relevant to describing a business/service provider.\"\"\"\n",
        "    first_line: int = Field(..., ge=0, description=\"0-based inclusive start line\")\n",
        "    last_line: int = Field(..., ge=0, description=\"0-based inclusive end line\")\n",
        "\n",
        "class SliceSet(BaseModel):\n",
        "    \"\"\"Top-level container returned by the model.\"\"\"\n",
        "    slices: List[Slice] = Field(default_factory=list)\n",
        "\n",
        "# Existing utility function (updated for tests/ subdirectory)\n",
        "def load_prompt(prompt_name):\n",
        "    \"\"\"Load prompt from prompts directory\"\"\"\n",
        "    with open(f\"../prompts/{prompt_name}.md\", \"r\") as f:\n",
        "        return f.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_llm_local(model=\"gpt-oss:20b\", prompt_content=None, temperature=0, keep_alive=\"30m\", num_predict=2048):\n",
        "    \"\"\"Fixed LLM call that avoids ChatPromptTemplate issues with HTML content.\"\"\"\n",
        "    if prompt_content is None:\n",
        "        raise ValueError(\"Prompt content is required\")\n",
        "    \n",
        "    # Use direct LLM call instead of ChatPromptTemplate\n",
        "    llm = ChatOllama(\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "        keep_alive=keep_alive,\n",
        "        num_predict=num_predict,\n",
        "        format=\"json\"  # Encourage JSON output\n",
        "    )\n",
        "    \n",
        "    # Direct invocation with HumanMessage\n",
        "    response = llm.invoke([HumanMessage(content=prompt_content)])\n",
        "    \n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_html_slices_fixed(html_content: str, use_local_llm: bool = True) -> SliceSet:\n",
        "    \"\"\"Fixed HTML slicing function that uses the original generate_slices.md prompt\n",
        "    but avoids ChatPromptTemplate issues by injecting schema and using direct calls.\"\"\"\n",
        "\n",
        "    prompt = load_prompt(\"generate_slices\")\n",
        "    json_schema = SliceSet.model_json_schema()\n",
        "    prompt = prompt.format(html_content=html_content, output_schema=json_schema)\n",
        "    \n",
        "    try:\n",
        "        # Use the fixed LLM call approach\n",
        "        if use_local_llm:\n",
        "            response = call_llm_local(\n",
        "                model=\"gpt-oss:20b\",\n",
        "                prompt_content=prompt,\n",
        "                temperature=0\n",
        "            )\n",
        "        \n",
        "        if len(response.content.strip()) > 0:\n",
        "            content = response.content.strip()\n",
        "            \n",
        "            # Handle different response formats (existing logic)\n",
        "            if content.startswith('```json'):\n",
        "                start_idx = content.find('\\n', 7) + 1\n",
        "                end_idx = content.rfind('```')\n",
        "                if start_idx > 7 and end_idx > start_idx:\n",
        "                    content = content[start_idx:end_idx].strip()\n",
        "            elif content.startswith('```'):\n",
        "                start_idx = content.find('\\n', 3) + 1\n",
        "                end_idx = content.rfind('```')\n",
        "                if start_idx > 3 and end_idx > start_idx:\n",
        "                    content = content[start_idx:end_idx].strip()\n",
        "            \n",
        "            # Parse JSON with existing robust logic\n",
        "            try:\n",
        "                parsed_json = json.loads(content)\n",
        "                \n",
        "                # Validate and create SliceSet\n",
        "                html_lines = len(html_content.split('\\n'))\n",
        "                valid_slices = []\n",
        "                \n",
        "                for slice_data in parsed_json.get('slices', []):\n",
        "                    first_line = slice_data.get('first_line', 0)\n",
        "                    last_line = slice_data.get('last_line', 0)\n",
        "                    \n",
        "                    # Ensure valid range\n",
        "                    first_line = max(0, min(first_line, html_lines - 1))\n",
        "                    last_line = max(first_line, min(last_line, html_lines - 1))\n",
        "                    \n",
        "                    valid_slices.append(Slice(first_line=first_line, last_line=last_line))\n",
        "                \n",
        "                if valid_slices:\n",
        "                    print(f\"✅ Successfully generated {len(valid_slices)} slices\")\n",
        "                    return SliceSet(slices=valid_slices)\n",
        "                \n",
        "            except json.JSONDecodeError as json_err:\n",
        "                print(f\"⚠️  JSON parsing failed: {json_err}\")\n",
        "                print(f\"Raw content preview: {content[:200]}...\")\n",
        "                \n",
        "                # Try manual JSON extraction (existing logic)\n",
        "                start_idx = content.find('{')\n",
        "                if start_idx != -1:\n",
        "                    brace_count = 0\n",
        "                    end_idx = -1\n",
        "                    for i in range(start_idx, len(content)):\n",
        "                        if content[i] == '{':\n",
        "                            brace_count += 1\n",
        "                        elif content[i] == '}':\n",
        "                            brace_count -= 1\n",
        "                            if brace_count == 0:\n",
        "                                end_idx = i + 1\n",
        "                                break\n",
        "                    \n",
        "                    if end_idx != -1:\n",
        "                        json_content = content[start_idx:end_idx]\n",
        "                        try:\n",
        "                            parsed_json = json.loads(json_content)\n",
        "                            slice_set = SliceSet(**parsed_json)\n",
        "                            print(f\"✅ Manual JSON extraction successful: {len(slice_set.slices)} slices\")\n",
        "                            return slice_set\n",
        "                        except:\n",
        "                            pass\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"❌ LLM call failed: {e}\")\n",
        "        # Safe fallback\n",
        "        html_lines = len(html_content.split('\\n'))\n",
        "        return SliceSet(slices=[Slice(first_line=0, last_line=min(html_lines - 1, 20))])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Existing utility functions (unchanged for compatibility)\n",
        "def get_slices(html_content: str, slice_set: SliceSet) -> str:\n",
        "    \"\"\"Extract the relevant slices from HTML content based on SliceSet output.\"\"\"\n",
        "    if not slice_set.slices:\n",
        "        return \"\"\n",
        "    \n",
        "    html_lines = html_content.split('\\n')\n",
        "    extracted_slices = []\n",
        "    \n",
        "    for slice_obj in slice_set.slices:\n",
        "        start_line = max(0, slice_obj.first_line)\n",
        "        end_line = min(len(html_lines) - 1, slice_obj.last_line)\n",
        "        \n",
        "        if start_line <= end_line:\n",
        "            slice_content = '\\n'.join(html_lines[start_line:end_line + 1])\n",
        "            extracted_slices.append(slice_content)\n",
        "            extracted_slices.append(f\"\\n<!-- SLICE {slice_obj.first_line}-{slice_obj.last_line} -->\\n\")\n",
        "    \n",
        "    return '\\n'.join(extracted_slices)\n",
        "\n",
        "def get_slices_with_metadata(html_content: str, slice_set: SliceSet) -> dict:\n",
        "    \"\"\"Extract slices with additional metadata for debugging.\"\"\"\n",
        "    if not slice_set.slices:\n",
        "        return {\n",
        "            'sliced_content': '',\n",
        "            'slice_info': [],\n",
        "            'stats': {'total_slices': 0, 'total_lines_extracted': 0, 'original_lines': len(html_content.split('\\n'))}\n",
        "        }\n",
        "    \n",
        "    html_lines = html_content.split('\\n')\n",
        "    extracted_slices = []\n",
        "    slice_info = []\n",
        "    total_lines_extracted = 0\n",
        "    \n",
        "    for i, slice_obj in enumerate(slice_set.slices):\n",
        "        start_line = max(0, slice_obj.first_line)\n",
        "        end_line = min(len(html_lines) - 1, slice_obj.last_line)\n",
        "        \n",
        "        if start_line <= end_line:\n",
        "            slice_content = '\\n'.join(html_lines[start_line:end_line + 1])\n",
        "            lines_in_slice = end_line - start_line + 1\n",
        "            total_lines_extracted += lines_in_slice\n",
        "            \n",
        "            extracted_slices.append(slice_content)\n",
        "            extracted_slices.append(f\"\\n<!-- SLICE {i+1}: lines {slice_obj.first_line}-{slice_obj.last_line} ({lines_in_slice} lines) -->\\n\")\n",
        "            \n",
        "            slice_info.append({\n",
        "                'slice_number': i + 1,\n",
        "                'first_line': slice_obj.first_line,\n",
        "                'last_line': slice_obj.last_line,\n",
        "                'lines_count': lines_in_slice,\n",
        "                'preview': slice_content[:200] + '...' if len(slice_content) > 200 else slice_content\n",
        "            })\n",
        "    \n",
        "    return {\n",
        "        'sliced_content': '\\n'.join(extracted_slices),\n",
        "        'slice_info': slice_info,\n",
        "        'stats': {\n",
        "            'total_slices': len(slice_set.slices),\n",
        "            'total_lines_extracted': total_lines_extracted,\n",
        "            'original_lines': len(html_lines),\n",
        "            'compression_ratio': round((1 - total_lines_extracted / len(html_lines)) * 100, 2) if len(html_lines) > 0 else 0\n",
        "        }\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Fixed Pipeline\n",
        "\n",
        "Let's test the fixed implementation with realistic HTML content:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing fixed HTML slicing pipeline...\n",
            "Input HTML: 3826 characters, 89 lines\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Test with realistic business HTML\n",
        "test_html = \"\"\"<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>TechConsult Pro - Digital Transformation Experts</title>\n",
        "    <meta name=\"description\" content=\"Leading digital transformation consultancy helping businesses modernize their operations and technology stack.\">\n",
        "    <link rel=\"stylesheet\" href=\"styles.css\">\n",
        "</head>\n",
        "<body>\n",
        "    <header class=\"main-header\">\n",
        "        <div class=\"container\">\n",
        "            <img src=\"logo.png\" alt=\"TechConsult Pro Logo\" class=\"logo\">\n",
        "            <h1>TechConsult Pro</h1>\n",
        "            <p class=\"tagline\">Transforming Businesses Through Technology</p>\n",
        "        </div>\n",
        "    </header>\n",
        "    \n",
        "    <nav class=\"main-nav\">\n",
        "        <ul>\n",
        "            <li><a href=\"#home\">Home</a></li>\n",
        "            <li><a href=\"#about\">About</a></li>\n",
        "            <li><a href=\"#services\">Services</a></li>\n",
        "            <li><a href=\"#contact\">Contact</a></li>\n",
        "        </ul>\n",
        "    </nav>\n",
        "    \n",
        "    <main>\n",
        "        <section id=\"about\" class=\"about-section\">\n",
        "            <div class=\"container\">\n",
        "                <h2>About TechConsult Pro</h2>\n",
        "                <p>With over 15 years of experience in digital transformation, we help enterprises navigate the complex landscape of modern technology. Our team of certified experts specializes in cloud migration, AI implementation, and digital strategy.</p>\n",
        "                <p>We've successfully transformed over 200 businesses across various industries, from startups to Fortune 500 companies.</p>\n",
        "            </div>\n",
        "        </section>\n",
        "        \n",
        "        <section id=\"services\" class=\"services-section\">\n",
        "            <div class=\"container\">\n",
        "                <h2>Our Services</h2>\n",
        "                <div class=\"service-grid\">\n",
        "                    <div class=\"service-item\">\n",
        "                        <h3>Cloud Migration</h3>\n",
        "                        <p>Seamless migration to AWS, Azure, or Google Cloud with zero downtime.</p>\n",
        "                        <p class=\"price\">Starting at $15,000</p>\n",
        "                    </div>\n",
        "                    <div class=\"service-item\">\n",
        "                        <h3>AI & Machine Learning</h3>\n",
        "                        <p>Custom AI solutions to automate processes and gain insights from your data.</p>\n",
        "                        <p class=\"price\">Starting at $25,000</p>\n",
        "                    </div>\n",
        "                    <div class=\"service-item\">\n",
        "                        <h3>Digital Strategy Consulting</h3>\n",
        "                        <p>Comprehensive digital roadmaps tailored to your business objectives.</p>\n",
        "                        <p class=\"price\">Starting at $8,000</p>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "        </section>\n",
        "        \n",
        "        <section id=\"contact\" class=\"contact-section\">\n",
        "            <div class=\"container\">\n",
        "                <h2>Get in Touch</h2>\n",
        "                <div class=\"contact-info\">\n",
        "                    <p><strong>Address:</strong> 1200 Tech Plaza, Suite 400, San Francisco, CA 94105</p>\n",
        "                    <p><strong>Phone:</strong> +1 (555) 123-4567</p>\n",
        "                    <p><strong>Email:</strong> contact@techconsultpro.com</p>\n",
        "                    <p><strong>Business Hours:</strong> Mon-Fri 9:00 AM - 6:00 PM PST</p>\n",
        "                </div>\n",
        "                <div class=\"social-links\">\n",
        "                    <a href=\"https://linkedin.com/company/techconsultpro\">LinkedIn</a>\n",
        "                    <a href=\"https://twitter.com/techconsultpro\">Twitter</a>\n",
        "                </div>\n",
        "            </div>\n",
        "        </section>\n",
        "    </main>\n",
        "    \n",
        "    <footer>\n",
        "        <p>&copy; 2024 TechConsult Pro. All rights reserved.</p>\n",
        "    </footer>\n",
        "    \n",
        "    <script src=\"analytics.js\"></script>\n",
        "    <script>\n",
        "        // Contact form handling\n",
        "        document.addEventListener('DOMContentLoaded', function() {\n",
        "            console.log('Page loaded');\n",
        "        });\n",
        "    </script>\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "print(\"Testing fixed HTML slicing pipeline...\")\n",
        "print(f\"Input HTML: {len(test_html)} characters, {len(test_html.split(chr(10)))} lines\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ SLICING SUCCESSFUL!\n",
            "❌ Pipeline failed: 'NoneType' object has no attribute 'slices'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_90669/3737143613.py\", line 6, in <module>\n",
            "    print(f\"Generated {len(slice_result.slices)} slices:\")\n",
            "                           ^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'slices'\n"
          ]
        }
      ],
      "source": [
        "# Run the fixed slicing pipeline\n",
        "try:\n",
        "    slice_result = generate_html_slices_fixed(test_html, use_local_llm=True)\n",
        "    \n",
        "    print(f\"\\n✅ SLICING SUCCESSFUL!\")\n",
        "    print(f\"Generated {len(slice_result.slices)} slices:\")\n",
        "    \n",
        "    # Get detailed results\n",
        "    detailed_result = get_slices_with_metadata(test_html, slice_result)\n",
        "    \n",
        "    print(\"\\n📊 Statistics:\")\n",
        "    for key, value in detailed_result['stats'].items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    \n",
        "    print(\"\\n📋 Slice Details:\")\n",
        "    for slice_info in detailed_result['slice_info']:\n",
        "        print(f\"  Slice {slice_info['slice_number']}: lines {slice_info['first_line']}-{slice_info['last_line']} ({slice_info['lines_count']} lines)\")\n",
        "        print(f\"    Preview: {slice_info['preview'][:150]}...\")\n",
        "        print()\n",
        "    \n",
        "    print(\"\\n📄 Extracted Content (first 1000 chars):\")\n",
        "    print(\"-\" * 50)\n",
        "    extracted_content = get_slices(test_html, slice_result)\n",
        "    print(extracted_content[:1000] + \"...\" if len(extracted_content) > 1000 else extracted_content)\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Pipeline failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sending prompt to LLM with HTML content and schema...\n",
            "\n",
            "Raw LLM response:\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Debug: Prompt the LLM directly with the HTML content and print raw response\n",
        "html_content = test_html  # reuse the test HTML from above\n",
        "\n",
        "prompt = load_prompt(\"generate_slices\")\n",
        "json_schema = SliceSet.model_json_schema()\n",
        "prompt = prompt.format(html_content=html_content, output_schema=json_schema)\n",
        "\n",
        "print(\"Sending prompt to LLM with HTML content and schema...\")\n",
        "try:\n",
        "    response = call_llm_local(model=\"gpt-oss:20b\", prompt_content=prompt, temperature=0)\n",
        "    print(\"\\nRaw LLM response:\\n\")\n",
        "    print(response.content if hasattr(response, \"content\") else response)\n",
        "except Exception as e:\n",
        "    print(f\"LLM call error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base URL: http://localhost:11434\n",
            "/api/tags status: 200\n",
            "Tags keys: ['models']\n",
            "Discovered models: ['gpt-oss:20b']\n",
            "Selected model: gpt-oss:20b\n",
            "Minimal chat took 0.40s, content: 'ready'\n",
            "Generate took 0.38s, response: 'Hello! How can I help you today?'\n",
            "ChatOllama ping took 0.46s\n",
            "Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
            "Content: 'ok'\n",
            "Additional kwargs: {}\n",
            "Response metadata: {'model': 'gpt-oss:20b', 'created_at': '2025-09-09T08:24:41.002132432Z', 'done': True, 'done_reason': 'stop', 'total_duration': 461649784, 'load_duration': 70430807, 'prompt_eval_count': 71, 'prompt_eval_duration': 28757062, 'eval_count': 42, 'eval_duration': 361889666, 'model_name': 'gpt-oss:20b'}\n"
          ]
        }
      ],
      "source": [
        "# Deep diagnostics: verify Ollama is reachable and models are available\n",
        "import os, time, json, traceback\n",
        "import requests\n",
        "\n",
        "# 1) Check Ollama base URL and health\n",
        "base_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')\n",
        "print(f\"Base URL: {base_url}\")\n",
        "try:\n",
        "    r = requests.get(f\"{base_url}/api/tags\", timeout=3)\n",
        "    print(f\"/api/tags status: {r.status_code}\")\n",
        "    tags = r.json() if r.ok else {}\n",
        "    print(f\"Tags keys: {list(tags.keys())}\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to reach Ollama: {e}\")\n",
        "    tags = {}\n",
        "\n",
        "# 2) Collect model names from tags + running processes\n",
        "model_names = []\n",
        "try:\n",
        "    from itertools import chain\n",
        "    import ollama\n",
        "    client = ollama.Client(host=base_url)\n",
        "    # from /api/tags\n",
        "    tag_models = [m.get('name') or m.get('model') or m.get('tag') for m in (tags.get('models') or [])]\n",
        "    # from ps (running models)\n",
        "    try:\n",
        "        ps = client.ps()\n",
        "        ps_models = [m.get('name') or m.get('model') for m in ps.get('models', [])]\n",
        "    except Exception:\n",
        "        ps = {}\n",
        "        ps_models = []\n",
        "    # from client.list()\n",
        "    try:\n",
        "        listed = client.list()\n",
        "        list_models = []\n",
        "        for m in listed.get('models', []):\n",
        "            if isinstance(m, dict):\n",
        "                list_models.append(m.get('name') or m.get('model'))\n",
        "            else:\n",
        "                # object-like fallback\n",
        "                list_models.append(getattr(m, 'name', None) or getattr(m, 'model', None))\n",
        "    except Exception:\n",
        "        list_models = []\n",
        "    model_names = [n for n in chain(tag_models, ps_models, list_models) if isinstance(n, str) and n]\n",
        "    # Ensure uniqueness, preserve order\n",
        "    seen = set()\n",
        "    model_names = [x for x in model_names if not (x in seen or seen.add(x))]\n",
        "    print(f\"Discovered models: {model_names}\")\n",
        "except Exception as e:\n",
        "    print(f\"Model discovery failed: {e}\")\n",
        "\n",
        "# 3) Pick a model, prefer gpt-oss:20b if present, else first discovered, else fallback\n",
        "preferred = 'gpt-oss:20b'\n",
        "selected_model = preferred if preferred in model_names else (model_names[0] if model_names else preferred)\n",
        "print(f\"Selected model: {selected_model}\")\n",
        "\n",
        "# 4) Minimal chat test\n",
        "try:\n",
        "    import ollama\n",
        "    client = ollama.Client(host=base_url)\n",
        "    t0 = time.time()\n",
        "    resp = client.chat(model=selected_model, messages=[{\"role\":\"user\", \"content\":\"Say 'ready'\"}], options={\"temperature\":0})\n",
        "    dt = time.time() - t0\n",
        "    content = resp.get('message', {}).get('content') if isinstance(resp, dict) else getattr(getattr(resp, 'message', {}), 'content', '')\n",
        "    print(f\"Minimal chat took {dt:.2f}s, content: {repr(content)[:200]}\")\n",
        "except Exception as e:\n",
        "    print(f\"Minimal chat failed: {e}\")\n",
        "\n",
        "# 5) Minimal generate test\n",
        "try:\n",
        "    t0 = time.time()\n",
        "    gen = client.generate(model=selected_model, prompt=\"Hello\", options={\"temperature\":0})\n",
        "    dt = time.time() - t0\n",
        "    txt = gen.get('response', '') if isinstance(gen, dict) else getattr(gen, 'response', '')\n",
        "    print(f\"Generate took {dt:.2f}s, response: {repr(txt)[:200]}\")\n",
        "except Exception as e:\n",
        "    print(f\"Generate failed: {e}\")\n",
        "\n",
        "# 6) Re-run ChatOllama with base_url and introspect the response object\n",
        "try:\n",
        "    from langchain_ollama import ChatOllama\n",
        "    from langchain_core.messages import HumanMessage\n",
        "    llm = ChatOllama(model=selected_model, base_url=base_url, temperature=0)\n",
        "    t0 = time.time()\n",
        "    resp2 = llm.invoke([HumanMessage(content=\"Say 'ok'\")])\n",
        "    dt = time.time() - t0\n",
        "    print(f\"ChatOllama ping took {dt:.2f}s\")\n",
        "    print(\"Type:\", type(resp2))\n",
        "    try:\n",
        "        print(\"Content:\", repr(getattr(resp2, 'content', None)))\n",
        "        print(\"Additional kwargs:\", getattr(resp2, 'additional_kwargs', None))\n",
        "        print(\"Response metadata:\", getattr(resp2, 'response_metadata', None))\n",
        "    except Exception:\n",
        "        traceback.print_exc()\n",
        "except Exception as e:\n",
        "    print(f\"ChatOllama ping failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Ollama at: http://localhost:11434 | model: gpt-oss:20b\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Improved base URL and model selection\n",
        "\n",
        "def _get_ollama_base_url() -> str:\n",
        "    return os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
        "\n",
        "\n",
        "def _select_model(preferred: list[str] | None = None) -> str:\n",
        "    preferred = preferred or [\"gpt-oss:20b\", \"codellama:34b\", \"llama3.1:8b-instruct\", \"llama3.2:3b-instruct\"]\n",
        "    try:\n",
        "        import ollama\n",
        "        client = ollama.Client(host=_get_ollama_base_url())\n",
        "        info = client.list()\n",
        "        names = [m.get(\"name\") for m in info.get(\"models\", []) if m.get(\"name\")]\n",
        "        for cand in preferred:\n",
        "            if cand in names:\n",
        "                return cand\n",
        "        if names:\n",
        "            return names[0]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return preferred[0]\n",
        "\n",
        "\n",
        "# Redefine call_llm_local with base_url and fallback\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "class _ContentWrapper:\n",
        "    def __init__(self, content: str):\n",
        "        self.content = content\n",
        "        self.response_metadata = {\"source\": \"ollama.direct\"}\n",
        "        self.additional_kwargs = {}\n",
        "\n",
        "\n",
        "def call_llm_local(model: str | None = None, prompt_content: str | None = None, temperature: float = 0, keep_alive: str = \"30m\", num_predict: int = 2048):\n",
        "    if prompt_content is None:\n",
        "        raise ValueError(\"Prompt content is required\")\n",
        "    base_url = _get_ollama_base_url()\n",
        "    model = model or _select_model()\n",
        "\n",
        "    try:\n",
        "        llm = ChatOllama(\n",
        "            model=model,\n",
        "            base_url=base_url,\n",
        "            temperature=temperature,\n",
        "            keep_alive=keep_alive,\n",
        "            num_predict=num_predict,\n",
        "            format=\"json\",\n",
        "        )\n",
        "        resp = llm.invoke([HumanMessage(content=prompt_content)])\n",
        "        content = (getattr(resp, \"content\", None) or \"\").strip()\n",
        "        if content:\n",
        "            return resp\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Fallback: direct Ollama client\n",
        "    try:\n",
        "        import ollama\n",
        "        client = ollama.Client(host=base_url)\n",
        "        chat = client.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt_content}], options={\"temperature\": temperature})\n",
        "        if isinstance(chat, dict):\n",
        "            content = (chat.get(\"message\", {}).get(\"content\") or \"\").strip()\n",
        "        else:\n",
        "            content = (getattr(getattr(chat, \"message\", {}), \"content\", None) or \"\").strip()\n",
        "        return _ContentWrapper(content)\n",
        "    except Exception as e:\n",
        "        return _ContentWrapper(\"\")\n",
        "\n",
        "\n",
        "# Redefine generate_html_slices_fixed to always return a SliceSet\n",
        "\n",
        "def generate_html_slices_fixed(html_content: str, use_local_llm: bool = True) -> SliceSet:\n",
        "    prompt = load_prompt(\"generate_slices\")\n",
        "    json_schema = SliceSet.model_json_schema()\n",
        "    prompt = prompt.format(html_content=html_content, output_schema=json_schema)\n",
        "\n",
        "    try:\n",
        "        response = None\n",
        "        if use_local_llm:\n",
        "            response = call_llm_local(model=_select_model(), prompt_content=prompt, temperature=0)\n",
        "        raw = (getattr(response, \"content\", None) or \"\").strip()\n",
        "        if not raw:\n",
        "            print(\"⚠️ Empty LLM content. Check diagnostics above.\")\n",
        "            return SliceSet(slices=[])\n",
        "\n",
        "        content = raw\n",
        "        if content.startswith('```json'):\n",
        "            start_idx = content.find('\\n', 7) + 1\n",
        "            end_idx = content.rfind('```')\n",
        "            if start_idx > 7 and end_idx > start_idx:\n",
        "                content = content[start_idx:end_idx].strip()\n",
        "        elif content.startswith('```'):\n",
        "            start_idx = content.find('\\n', 3) + 1\n",
        "            end_idx = content.rfind('```')\n",
        "            if start_idx > 3 and end_idx > start_idx:\n",
        "                content = content[start_idx:end_idx].strip()\n",
        "\n",
        "        try:\n",
        "            parsed_json = json.loads(content)\n",
        "            html_lines = len(html_content.split('\\n'))\n",
        "            valid_slices = []\n",
        "            for slice_data in parsed_json.get('slices', []):\n",
        "                first_line = max(0, min(int(slice_data.get('first_line', 0)), html_lines - 1))\n",
        "                last_line = max(first_line, min(int(slice_data.get('last_line', 0)), html_lines - 1))\n",
        "                valid_slices.append(Slice(first_line=first_line, last_line=last_line))\n",
        "            print(f\"✅ Parsed {len(valid_slices)} slices\")\n",
        "            return SliceSet(slices=valid_slices)\n",
        "        except json.JSONDecodeError as json_err:\n",
        "            print(f\"⚠️ JSON parsing failed: {json_err}\")\n",
        "            start_idx = content.find('{')\n",
        "            if start_idx != -1:\n",
        "                brace_count = 0\n",
        "                end_idx = -1\n",
        "                for i in range(start_idx, len(content)):\n",
        "                    if content[i] == '{':\n",
        "                        brace_count += 1\n",
        "                    elif content[i] == '}':\n",
        "                        brace_count -= 1\n",
        "                        if brace_count == 0:\n",
        "                            end_idx = i + 1\n",
        "                            break\n",
        "                if end_idx != -1:\n",
        "                    try:\n",
        "                        parsed_json = json.loads(content[start_idx:end_idx])\n",
        "                        slice_set = SliceSet(**parsed_json)\n",
        "                        print(f\"✅ Manual JSON extraction: {len(slice_set.slices)} slices\")\n",
        "                        return slice_set\n",
        "                    except Exception:\n",
        "                        pass\n",
        "    except Exception as e:\n",
        "        print(f\"❌ LLM call failed: {e}\")\n",
        "        html_lines = len(html_content.split('\\n'))\n",
        "        return SliceSet(slices=[Slice(first_line=0, last_line=min(html_lines - 1, 20))])\n",
        "\n",
        "    # Final safe return if nothing matched\n",
        "    return SliceSet(slices=[]) \n",
        "\n",
        "print(f\"Using Ollama at: {_get_ollama_base_url()} | model: {_select_model()}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "growbal",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
