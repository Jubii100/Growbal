{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Minimal Fixed HTML Slicing Pipeline\n",
        "\n",
        "This notebook provides a robust slicing pipeline that:\n",
        "- Calls Ollama directly with a selected model\n",
        "- Always returns a `SliceSet` (empty on failure) to avoid `NoneType` errors\n",
        "- Prints raw LLM output for debugging\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, time\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Pydantic models\n",
        "class Slice(BaseModel):\n",
        "    first_line: int = Field(..., ge=0)\n",
        "    last_line: int = Field(..., ge=0)\n",
        "\n",
        "class SliceSet(BaseModel):\n",
        "    slices: List[Slice] = Field(default_factory=list)\n",
        "\n",
        "# Utilities\n",
        "\n",
        "def load_prompt(prompt_name: str) -> str:\n",
        "    with open(f\"../prompts/{prompt_name}.md\", \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def _get_ollama_base_url() -> str:\n",
        "    return os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
        "\n",
        "\n",
        "def _select_model(preferred: list[str] | None = None) -> str:\n",
        "    preferred = preferred or [\"gpt-oss:20b\"]\n",
        "    try:\n",
        "        import ollama\n",
        "        client = ollama.Client(host=_get_ollama_base_url())\n",
        "        # Prefer running models\n",
        "        try:\n",
        "            ps = client.ps()\n",
        "            running = [m.get('name') or m.get('model') for m in ps.get('models', []) if isinstance(m, dict)]\n",
        "        except Exception:\n",
        "            running = []\n",
        "        # Fallback to listed\n",
        "        try:\n",
        "            listed = client.list()\n",
        "            listed_models = [m.get('name') or m.get('model') for m in listed.get('models', []) if isinstance(m, dict)]\n",
        "        except Exception:\n",
        "            listed_models = []\n",
        "        names = [n for n in running + listed_models if isinstance(n, str) and n]\n",
        "        # prefer desired\n",
        "        for cand in (preferred or []):\n",
        "            if cand in names:\n",
        "                return cand\n",
        "        if names:\n",
        "            return names[0]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return preferred[0]\n",
        "\n",
        "\n",
        "# LLM call with direct Ollama and safe wrapper\n",
        "class _ContentWrapper:\n",
        "    def __init__(self, content: str, raw_response=None, model: str = \"\", duration_s: float = 0.0):\n",
        "        self.content = content\n",
        "        self.raw_response = raw_response\n",
        "        self.model = model\n",
        "        self.duration_s = duration_s\n",
        "        self.response_metadata = {\"source\": \"ollama.direct\"}\n",
        "        self.additional_kwargs = {}\n",
        "\n",
        "\n",
        "def call_llm_local(prompt_content: str, model: str | None = None, temperature: float = 0, schema: dict | None = None) -> _ContentWrapper:\n",
        "    if not isinstance(prompt_content, str) or not prompt_content.strip():\n",
        "        return _ContentWrapper(\"\")\n",
        "    model = model or _select_model()\n",
        "    base_url = _get_ollama_base_url()\n",
        "    try:\n",
        "        import ollama\n",
        "        client = ollama.Client(host=base_url)\n",
        "        t0 = time.time()\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt_content}]\n",
        "        tools = None\n",
        "        if schema:\n",
        "            tools = [{\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": \"structured_response\",\n",
        "                    \"description\": \"Provide a structured response according to the schema\",\n",
        "                    \"parameters\": schema\n",
        "                }\n",
        "            }]\n",
        "            # Strong instruction to use the tool\n",
        "            messages.insert(0, {\"role\": \"system\", \"content\": \"You must respond using the structured_response function with the exact schema. Do not include any other text.\"})\n",
        "        resp = client.chat(model=model, messages=messages, tools=tools, options={\"temperature\": temperature})\n",
        "        dt = time.time() - t0\n",
        "        # Extract content and tool_calls if present\n",
        "        content = resp.get('message', {}).get('content', '') if isinstance(resp, dict) else ''\n",
        "        tool_calls = resp.get('message', {}).get('tool_calls', []) if isinstance(resp, dict) else []\n",
        "        print(f\"LLM chat took {dt:.2f}s | model={model}\")\n",
        "        print(\"RAW ollama.chat response:\")\n",
        "        try:\n",
        "            print(json.dumps(resp, ensure_ascii=False, indent=2))\n",
        "        except Exception:\n",
        "            print(str(resp))\n",
        "        # If content empty but tool_calls present, pack the tool args as content for downstream parsing\n",
        "        if (not content.strip()) and tool_calls:\n",
        "            try:\n",
        "                args = tool_calls[0].get('function', {}).get('arguments')\n",
        "                if isinstance(args, str):\n",
        "                    content = args\n",
        "                elif isinstance(args, dict):\n",
        "                    content = json.dumps(args)\n",
        "            except Exception:\n",
        "                pass\n",
        "        return _ContentWrapper((content or '').strip(), raw_response=resp, model=model, duration_s=dt)\n",
        "    except Exception as e:\n",
        "        print(f\"LLM call error: {e}\")\n",
        "        return _ContentWrapper(\"\", raw_response=None, model=model or \"\", duration_s=0.0)\n",
        "\n",
        "\n",
        "def generate_html_slices_fixed(html_content: str) -> SliceSet:\n",
        "    prompt = load_prompt(\"generate_slices\")\n",
        "    json_schema = SliceSet.model_json_schema()\n",
        "    prompt = prompt.format(html_content=html_content, output_schema=json_schema)\n",
        "\n",
        "    response = call_llm_local(prompt, schema=json_schema)\n",
        "    raw = (getattr(response, \"content\", None) or \"\").strip()\n",
        "    print(\"Raw LLM response (full):\\n\", raw)\n",
        "    # Also print raw response object for complete debugging\n",
        "    try:\n",
        "        print(\"\\nRaw response object:\")\n",
        "        print(json.dumps(response.raw_response, ensure_ascii=False, indent=2))\n",
        "    except Exception:\n",
        "        print(response.raw_response)\n",
        "    if not raw:\n",
        "        return SliceSet(slices=[])\n",
        "\n",
        "    content = raw\n",
        "    if content.startswith('```json'):\n",
        "        start_idx = content.find('\\n', 7) + 1\n",
        "        end_idx = content.rfind('```')\n",
        "        if start_idx > 7 and end_idx > start_idx:\n",
        "            content = content[start_idx:end_idx].strip()\n",
        "    elif content.startswith('```'):\n",
        "        start_idx = content.find('\\n', 3) + 1\n",
        "        end_idx = content.rfind('```')\n",
        "        if start_idx > 3 and end_idx > start_idx:\n",
        "            content = content[start_idx:end_idx].strip()\n",
        "\n",
        "    try:\n",
        "        data = json.loads(content)\n",
        "    except Exception as e:\n",
        "        # Try to extract JSON substring\n",
        "        start = content.find('{')\n",
        "        end = content.rfind('}')\n",
        "        if start != -1 and end != -1 and end > start:\n",
        "            try:\n",
        "                data = json.loads(content[start:end+1])\n",
        "            except Exception:\n",
        "                data = {}\n",
        "        else:\n",
        "            data = {}\n",
        "\n",
        "    html_lines = html_content.split('\\n')\n",
        "    valid: List[Slice] = []\n",
        "    for sl in data.get('slices', []) if isinstance(data, dict) else []:\n",
        "        try:\n",
        "            a = int(sl.get('first_line', 0))\n",
        "            b = int(sl.get('last_line', 0))\n",
        "        except Exception:\n",
        "            continue\n",
        "        a = max(0, min(a, len(html_lines) - 1))\n",
        "        b = max(a, min(b, len(html_lines) - 1))\n",
        "        valid.append(Slice(first_line=a, last_line=b))\n",
        "\n",
        "    return SliceSet(slices=valid)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input HTML: 459 chars, 17 lines\n",
            "LLM chat took 4.01s | model=gpt-oss:20b\n",
            "RAW ollama.chat response:\n",
            "model='gpt-oss:20b' created_at='2025-09-09T08:35:56.431697371Z' done=True done_reason='stop' total_duration=4004240803 load_duration=69008023 prompt_eval_count=751 prompt_eval_duration=591615039 eval_count=362 eval_duration=3341029679 message=Message(role='assistant', content='', thinking='We need to output JSON with slices. Input lines: Let\\'s enumerate lines. The HTML snippet:\\n\\nLine 0: <!DOCTYPE html>\\n1: <html>\\n2: <head><title>Example Co - Services</title></head>\\n3: <body>\\n4: <h1>Example Co</h1>\\n5: <p>We provide consulting, cloud migration, and AI services.</p>\\n6: <section id=\"services\">\\n7:   <h2>Services</h2>\\n8:   <div><h3>Cloud Migration</h3><p>Move to AWS, Azure, or GCP.</p></div>\\n9:   <div><h3>AI</h3><p>Custom ML solutions for automation.</p></div>\\n10: </section>\\n11: <section id=\"contact\">\\n12:   <h2>Contact</h2>\\n13:   <p>Email: info@example.com</p>\\n14: </section>\\n15: </body>\\n16: </html>\\n\\nWe need slices that describe business/service provider. Relevant content: brand name, tagline, services, contact. So lines 4-5, 6-10, 11-13. Merge adjacent slices logically. We can combine 4-5 as one slice. 6-10 as services. 11-13 as contact. So slices: [4,5], [6,10], [11,13]. Ensure inclusive. Output JSON accordingly.', images=None, tool_name=None, tool_calls=[ToolCall(function=Function(name='structured_response', arguments={'slices': [{'first_line': 4, 'last_line': 5}, {'first_line': 6, 'last_line': 10}, {'first_line': 11, 'last_line': 13}]}))])\n",
            "Raw LLM response (full):\n",
            " \n",
            "\n",
            "Raw response object:\n",
            "model='gpt-oss:20b' created_at='2025-09-09T08:35:56.431697371Z' done=True done_reason='stop' total_duration=4004240803 load_duration=69008023 prompt_eval_count=751 prompt_eval_duration=591615039 eval_count=362 eval_duration=3341029679 message=Message(role='assistant', content='', thinking='We need to output JSON with slices. Input lines: Let\\'s enumerate lines. The HTML snippet:\\n\\nLine 0: <!DOCTYPE html>\\n1: <html>\\n2: <head><title>Example Co - Services</title></head>\\n3: <body>\\n4: <h1>Example Co</h1>\\n5: <p>We provide consulting, cloud migration, and AI services.</p>\\n6: <section id=\"services\">\\n7:   <h2>Services</h2>\\n8:   <div><h3>Cloud Migration</h3><p>Move to AWS, Azure, or GCP.</p></div>\\n9:   <div><h3>AI</h3><p>Custom ML solutions for automation.</p></div>\\n10: </section>\\n11: <section id=\"contact\">\\n12:   <h2>Contact</h2>\\n13:   <p>Email: info@example.com</p>\\n14: </section>\\n15: </body>\\n16: </html>\\n\\nWe need slices that describe business/service provider. Relevant content: brand name, tagline, services, contact. So lines 4-5, 6-10, 11-13. Merge adjacent slices logically. We can combine 4-5 as one slice. 6-10 as services. 11-13 as contact. So slices: [4,5], [6,10], [11,13]. Ensure inclusive. Output JSON accordingly.', images=None, tool_name=None, tool_calls=[ToolCall(function=Function(name='structured_response', arguments={'slices': [{'first_line': 4, 'last_line': 5}, {'first_line': 6, 'last_line': 10}, {'first_line': 11, 'last_line': 13}]}))])\n",
            "\n",
            "✅ SLICING SUCCESSFUL!\n",
            "Generated 0 slices:\n",
            "\n",
            "Extracted (first 800 chars):\n",
            " \n"
          ]
        }
      ],
      "source": [
        "# Test HTML\n",
        "html_sample = \"\"\"<!DOCTYPE html>\n",
        "<html>\n",
        "<head><title>Example Co - Services</title></head>\n",
        "<body>\n",
        "<h1>Example Co</h1>\n",
        "<p>We provide consulting, cloud migration, and AI services.</p>\n",
        "<section id=\"services\">\n",
        "  <h2>Services</h2>\n",
        "  <div><h3>Cloud Migration</h3><p>Move to AWS, Azure, or GCP.</p></div>\n",
        "  <div><h3>AI</h3><p>Custom ML solutions for automation.</p></div>\n",
        "</section>\n",
        "<section id=\"contact\">\n",
        "  <h2>Contact</h2>\n",
        "  <p>Email: info@example.com</p>\n",
        "</section>\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "print(f\"Input HTML: {len(html_sample)} chars, {len(html_sample.split(chr(10)))} lines\")\n",
        "\n",
        "# Run slicing\n",
        "slice_result = generate_html_slices_fixed(html_sample)\n",
        "print(\"\\n✅ SLICING SUCCESSFUL!\")\n",
        "print(f\"Generated {len(slice_result.slices)} slices:\")\n",
        "\n",
        "# Extract and show\n",
        "def get_slices(html_content: str, slice_set: SliceSet) -> str:\n",
        "    if not slice_set.slices:\n",
        "        return \"\"\n",
        "    lines = html_content.split('\\n')\n",
        "    chunks = []\n",
        "    for sl in slice_set.slices:\n",
        "        a = max(0, sl.first_line)\n",
        "        b = min(len(lines) - 1, sl.last_line)\n",
        "        if a <= b:\n",
        "            chunks.append('\\n'.join(lines[a:b+1]))\n",
        "            chunks.append(f\"\\n<!-- SLICE {sl.first_line}-{sl.last_line} -->\\n\")\n",
        "    return '\\n'.join(chunks)\n",
        "\n",
        "extracted = get_slices(html_sample, slice_result)\n",
        "print(\"\\nExtracted (first 800 chars):\\n\", extracted[:800])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "growbal",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
