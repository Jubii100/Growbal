{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final HTML Slicing Pipeline (Function Calling + Logging)\n",
        "\n",
        "This notebook is a clean, working version:\n",
        "- Uses Ollama function calling with a `get_slices` tool\n",
        "- Passes the exact Pydantic JSON schema to the tool\n",
        "- Parses tool calls (dict or two-item list formats)\n",
        "- Always returns a `SliceSet` and executes `get_slices` locally\n",
        "- Logs: initial HTML, full LLM prompt, raw LLM output, final sliced HTML\n",
        "- Slices are separated by a single blank line (no markers)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, time\n",
        "from typing import List, Dict\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Models\n",
        "class Slice(BaseModel):\n",
        "    first_line: int = Field(..., ge=0)\n",
        "    last_line: int = Field(..., ge=0)\n",
        "\n",
        "class SliceSet(BaseModel):\n",
        "    slices: List[Slice] = Field(default_factory=list)\n",
        "\n",
        "# Paths and logging\n",
        "\n",
        "def _resolve_base_dir() -> str:\n",
        "    candidates = [os.getcwd(), os.path.dirname(os.getcwd())]\n",
        "    for c in candidates:\n",
        "        if os.path.isdir(os.path.join(c, \"crawler_v2\", \"prompts\")):\n",
        "            return os.path.join(c, \"crawler_v2\")\n",
        "        if os.path.isdir(os.path.join(c, \"prompts\")):\n",
        "            return c\n",
        "    return os.getcwd()\n",
        "\n",
        "BASE_DIR = _resolve_base_dir()\n",
        "LOG_DIR = os.path.join(BASE_DIR, \"logs\")\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "def _write_log(filename: str, content: str) -> str:\n",
        "    # Overwrite same filenames each run (no RUN_ID prefix)\n",
        "    path = os.path.join(LOG_DIR, filename)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content)\n",
        "    return path\n",
        "\n",
        "# Prompt loader\n",
        "\n",
        "def load_prompt(prompt_name: str) -> str:\n",
        "    path = os.path.join(BASE_DIR, \"prompts\", f\"{prompt_name}.md\")\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "# Slice rendering\n",
        "\n",
        "def get_slices(html_content: str, slice_set: SliceSet) -> str:\n",
        "    if not slice_set.slices:\n",
        "        return \"\"\n",
        "    lines = html_content.split('\\n')\n",
        "    chunks: List[str] = []\n",
        "    for sl in slice_set.slices:\n",
        "        a = max(0, sl.first_line)\n",
        "        b = min(len(lines) - 1, sl.last_line)\n",
        "        if a <= b:\n",
        "            chunks.append('\\n'.join(lines[a:b+1]))\n",
        "    return '\\n\\n'.join(chunks)\n",
        "\n",
        "# Ollama helpers\n",
        "\n",
        "def _get_ollama_base_url() -> str:\n",
        "    return os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
        "\n",
        "\n",
        "def _select_model(preferred: List[str] | None = None) -> str:\n",
        "    preferred = preferred or [\"gpt-oss:20b\", \"llama3.2:3b-instruct\"]\n",
        "    try:\n",
        "        import ollama\n",
        "        client = ollama.Client(host=_get_ollama_base_url())\n",
        "        try:\n",
        "            ps = client.ps()\n",
        "            running = [m.get('name') or m.get('model') for m in ps.get('models', []) if isinstance(m, dict)]\n",
        "        except Exception:\n",
        "            running = []\n",
        "        try:\n",
        "            listed = client.list()\n",
        "            listed_models = [m.get('name') or m.get('model') for m in listed.get('models', []) if isinstance(m, dict)]\n",
        "        except Exception:\n",
        "            listed_models = []\n",
        "        candidates = [n for n in running + listed_models if isinstance(n, str) and n]\n",
        "        for cand in (preferred or []):\n",
        "            if cand in candidates:\n",
        "                return cand\n",
        "        if candidates:\n",
        "            return candidates[0]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (preferred or [\"gpt-oss:20b\"])[0]\n",
        "\n",
        "# Core pipeline\n",
        "\n",
        "def generate_slices_via_tools(html_content: str) -> SliceSet:\n",
        "    _write_log(\"initial_html.txt\", html_content)\n",
        "\n",
        "    prompt_tmpl = load_prompt(\"generate_slices\")\n",
        "    schema = SliceSet.model_json_schema()\n",
        "    schema_json = json.dumps(schema, ensure_ascii=False)\n",
        "    prompt = prompt_tmpl.format(html_content=html_content, output_schema=schema_json)\n",
        "    _write_log(\"llm_prompt.txt\", prompt)\n",
        "\n",
        "    tool_schema = schema\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Respond ONLY by calling the get_slices function with arguments that EXACTLY match the provided JSON Schema. Do not output any other text.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    tools = [{\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_slices\",\n",
        "            \"description\": \"Return slice ranges to extract relevant HTML lines.\",\n",
        "            \"parameters\": tool_schema\n",
        "        }\n",
        "    }]\n",
        "\n",
        "    import ollama\n",
        "    client = ollama.Client(host=_get_ollama_base_url())\n",
        "    t0 = time.time()\n",
        "    resp = client.chat(model=_select_model(), messages=messages, tools=tools, options={\"temperature\": 0})\n",
        "    dt = time.time() - t0\n",
        "\n",
        "    try:\n",
        "        _write_log(\"llm_output.txt\", json.dumps(resp, ensure_ascii=False, indent=2))\n",
        "    except Exception:\n",
        "        _write_log(\"llm_output.txt\", str(resp))\n",
        "\n",
        "    # Extract args from tool calls (support dict and object-style responses)\n",
        "    args: Dict = {}\n",
        "    if isinstance(resp, dict):\n",
        "        tcs = resp.get('message', {}).get('tool_calls', [])\n",
        "        if tcs:\n",
        "            fn = tcs[0].get('function', {})\n",
        "            raw = fn.get('arguments')\n",
        "            if isinstance(raw, str):\n",
        "                try:\n",
        "                    args = json.loads(raw)\n",
        "                except Exception:\n",
        "                    args = {}\n",
        "            elif isinstance(raw, dict):\n",
        "                args = raw\n",
        "    else:\n",
        "        # Object-style response (e.g., with .message and .tool_calls attributes)\n",
        "        message = getattr(resp, 'message', None)\n",
        "        if message is not None:\n",
        "            tc_list = getattr(message, 'tool_calls', []) or []\n",
        "            if tc_list:\n",
        "                fn_obj = getattr(tc_list[0], 'function', None)\n",
        "                raw = getattr(fn_obj, 'arguments', None) if fn_obj is not None else None\n",
        "                if isinstance(raw, str):\n",
        "                    try:\n",
        "                        args = json.loads(raw)\n",
        "                    except Exception:\n",
        "                        args = {}\n",
        "                elif isinstance(raw, dict):\n",
        "                    args = raw\n",
        "\n",
        "    # Build SliceSet (handle dict or [start, end])\n",
        "    parsed_preview = []\n",
        "    slices: List[Slice] = []\n",
        "    for sl in (args.get('slices') or []):\n",
        "        a = None\n",
        "        b = None\n",
        "        if isinstance(sl, dict):\n",
        "            a = sl.get('first_line', None)\n",
        "            b = sl.get('last_line', None)\n",
        "        elif isinstance(sl, list) and len(sl) >= 2:\n",
        "            a, b = sl[0], sl[1]\n",
        "        try:\n",
        "            a = int(a)\n",
        "            b = int(b)\n",
        "        except Exception:\n",
        "            continue\n",
        "        a = max(0, a)\n",
        "        b = max(a, b)\n",
        "        slices.append(Slice(first_line=a, last_line=b))\n",
        "        parsed_preview.append([a, b])\n",
        "\n",
        "    _write_log(\"parsed_tool_args.txt\", json.dumps({\"slices\": parsed_preview}, ensure_ascii=False))\n",
        "\n",
        "    result = SliceSet(slices=slices)\n",
        "    final_html = get_slices(html_content, result)\n",
        "    _write_log(\"final_sliced_html.txt\", final_html)\n",
        "\n",
        "    print(f\"LLM + tools took {dt:.2f}s. Produced {len(result.slices)} slices.\")\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input HTML: 459 chars, 17 lines\n",
            "LLM + tools took 13.66s. Produced 3 slices.\n",
            "Slices: [{'first_line': 4, 'last_line': 5}, {'first_line': 6, 'last_line': 9}, {'first_line': 11, 'last_line': 13}]\n",
            "Logs written to: /home/mohammed/Desktop/tech_projects/growbal/crawler_v2/logs\n"
          ]
        }
      ],
      "source": [
        "# Test cell\n",
        "html_sample = \"\"\"<!DOCTYPE html>\n",
        "<html>\n",
        "<head><title>Example Co - Services</title></head>\n",
        "<body>\n",
        "<h1>Example Co</h1>\n",
        "<p>We provide consulting, cloud migration, and AI services.</p>\n",
        "<section id=\"services\">\n",
        "  <h2>Services</h2>\n",
        "  <div><h3>Cloud Migration</h3><p>Move to AWS, Azure, or GCP.</p></div>\n",
        "  <div><h3>AI</h3><p>Custom ML solutions for automation.</p></div>\n",
        "</section>\n",
        "<section id=\"contact\">\n",
        "  <h2>Contact</h2>\n",
        "  <p>Email: info@example.com</p>\n",
        "</section>\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "print(f\"Input HTML: {len(html_sample)} chars, {len(html_sample.split(chr(10)))} lines\")\n",
        "res = generate_slices_via_tools(html_sample)\n",
        "print(\"Slices:\", [s.model_dump() for s in res.slices])\n",
        "print(f\"Logs written to: {LOG_DIR}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "growbal",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
