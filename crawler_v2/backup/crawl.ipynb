{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f781b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from anthropic._exceptions import OverloadedError\n",
    "import os\n",
    "from pydantic import BaseModel, Field, model_validator\n",
    "from typing import List, Dict, Any\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pydantic import Field\n",
    "# from __future__ import print_function\n",
    "# from IPython.display import display\n",
    "from typing import Optional\n",
    "from utils import Country, ProviderType\n",
    "import anthropic\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "# import traceback\n",
    "\n",
    "# llm_model = os.environ[\"LLM_VERSION\"]\n",
    "\n",
    "# HTML Slicing Pipeline Models and Functions\n",
    "class Slice(BaseModel):\n",
    "    first_line: int = Field(..., ge=0)\n",
    "    last_line: int = Field(..., ge=0)\n",
    "\n",
    "class SliceSet(BaseModel):\n",
    "    slices: List[Slice] = Field(default_factory=list)\n",
    "\n",
    "# --- Simple, robust JSON extraction and slicing ---\n",
    "\n",
    "def _extract_json_from_text(text: str) -> Dict[str, Any]:\n",
    "    if not text:\n",
    "        return {}\n",
    "    s = text.strip()\n",
    "    # Remove common markdown fences\n",
    "    if s.startswith(\"```\") and s.endswith(\"```\"):\n",
    "        s = s.strip(\"`\")\n",
    "        s = s[s.find(\"\\n\") + 1:] if \"\\n\" in s else s\n",
    "    # Fast-path: direct JSON\n",
    "    try:\n",
    "        if s.startswith(\"{\") and s.endswith(\"}\"):\n",
    "            return json.loads(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Find a JSON object that mentions \"slices\" using brace scanning\n",
    "    start_idx = -1\n",
    "    depth = 0\n",
    "    candidate = None\n",
    "    for i, ch in enumerate(s):\n",
    "        if ch == '{':\n",
    "            if depth == 0:\n",
    "                start_idx = i\n",
    "            depth += 1\n",
    "        elif ch == '}':\n",
    "            if depth > 0:\n",
    "                depth -= 1\n",
    "                if depth == 0 and start_idx != -1:\n",
    "                    frag = s[start_idx:i+1]\n",
    "                    if '\"slices\"' in frag or \"'slices'\" in frag:\n",
    "                        candidate = frag\n",
    "                        break\n",
    "    if candidate:\n",
    "        try:\n",
    "            return json.loads(candidate)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Regex fallback: minimal object containing \"slices\"\n",
    "    try:\n",
    "        import re as _re\n",
    "        m = _re.search(r\"\\{[\\s\\S]*?\\\"slices\\\"[\\s\\S]*?\\}\", s)\n",
    "        if m:\n",
    "            return json.loads(m.group(0))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {}\n",
    "\n",
    "\n",
    "def _build_slice_set_from_args(args: Dict[str, Any]) -> SliceSet:\n",
    "    raw_slices = args.get(\"slices\") or []\n",
    "    parsed_preview: List[List[int]] = []\n",
    "    slices: List[Slice] = []\n",
    "    for sl in raw_slices:\n",
    "        first = None\n",
    "        last = None\n",
    "        if isinstance(sl, dict):\n",
    "            if 'first_line' in sl and 'last_line' in sl:\n",
    "                first, last = sl.get('first_line'), sl.get('last_line')\n",
    "            elif 'start' in sl and 'end' in sl:\n",
    "                first, last = sl.get('start'), sl.get('end')\n",
    "            elif 'first' in sl and 'last' in sl:\n",
    "                first, last = sl.get('first'), sl.get('last')\n",
    "        elif isinstance(sl, list) and len(sl) >= 2:\n",
    "            first, last = sl[0], sl[1]\n",
    "        try:\n",
    "            first = int(first)\n",
    "            last = int(last)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if first < 0:\n",
    "            first = 0\n",
    "        if last < first:\n",
    "            last = first\n",
    "        slices.append(Slice(first_line=first, last_line=last))\n",
    "        parsed_preview.append([first, last])\n",
    "    _write_log(\"parsed_tool_args.txt\", json.dumps({\"slices\": parsed_preview}, ensure_ascii=False))\n",
    "    return SliceSet(slices=slices)\n",
    "\n",
    "\n",
    "def generate_slices_simple(html_content: str) -> SliceSet:\n",
    "    \"\"\"Ask the model for JSON-only slices and parse into SliceSet.\"\"\"\n",
    "    # Truncate HTML for fast testing (kept from previous logic)\n",
    "    max_html_tokens = 4000\n",
    "    estimated_tokens = len(html_content) // 4\n",
    "    if estimated_tokens > max_html_tokens:\n",
    "        max_chars = max_html_tokens * 4\n",
    "        html_content = html_content[:max_chars]\n",
    "        last_newline = html_content.rfind('\\n')\n",
    "        if last_newline > max_chars * 0.9:\n",
    "            html_content = html_content[:last_newline]\n",
    "        print(f\"HTML truncated to {len(html_content)} chars (~{len(html_content)//4} tokens) for fast testing\")\n",
    "\n",
    "    _write_log(\"raw_html.txt\", html_content)\n",
    "\n",
    "    schema = SliceSet.model_json_schema()\n",
    "    schema_json = json.dumps(schema, ensure_ascii=False)\n",
    "    sys_prompt = (\n",
    "        \"You return ONLY JSON. No prose, no markdown, no code fences. \"\n",
    "        \"The JSON MUST exactly follow the provided schema. If nothing relevant, return {\\\"slices\\\": []}.\"\n",
    "    )\n",
    "    # user_prompt = (\n",
    "    #     f\"{load_prompt('generate_slices')}\\n\\n\"\n",
    "    #     f\"Schema:\\n{schema_json}\\n\\n\"\n",
    "    #     f\"Input HTML:\\n{html_content}\"\n",
    "    # )\n",
    "    prompt_tmpl = load_prompt(\"generate_slices\")\n",
    "    user_prompt = prompt_tmpl.format(html_content=html_content, output_schema=schema_json)\n",
    "\n",
    "    try:\n",
    "        import ollama\n",
    "        client = ollama.Client(host=_get_ollama_base_url())\n",
    "        t0 = time.time()\n",
    "        resp = client.chat(\n",
    "            model=_select_model(),\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            options={\n",
    "                \"temperature\": 0,\n",
    "                \"num_ctx\": 16384,\n",
    "                \"num_batch\": 1024,\n",
    "                \"mirostat\": 0,\n",
    "                \"repeat_penalty\": 1.1,\n",
    "                \"seed\": 12345,\n",
    "            },\n",
    "            keep_alive=\"0\",\n",
    "        )\n",
    "        dt = time.time() - t0\n",
    "        try:\n",
    "            ollama_response = json.dumps(resp, ensure_ascii=False, indent=2)\n",
    "        except Exception:\n",
    "            ollama_response = str(resp)\n",
    "        _write_log(\"ollama_prompt.txt\", user_prompt)\n",
    "        _write_log(\"ollama_response.txt\", f\"OLLAMA RESPONSE ({dt:.2f}s)\\n{ollama_response}\")\n",
    "\n",
    "        # Extract content text robustly\n",
    "        content = None\n",
    "        if isinstance(resp, dict):\n",
    "            msg = resp.get(\"message\") or {}\n",
    "            content = (msg.get(\"content\") if isinstance(msg, dict) else None) or resp.get(\"response\") or resp.get(\"content\")\n",
    "        else:\n",
    "            msg = getattr(resp, \"message\", None)\n",
    "            content = (getattr(msg, \"content\", None) if msg else None) or getattr(resp, \"response\", None) or getattr(resp, \"content\", None)\n",
    "        if isinstance(content, list):\n",
    "            # Some clients stream as list of parts; join strings\n",
    "            content = \"\".join([p if isinstance(p, str) else p.get(\"text\", \"\") for p in content])\n",
    "\n",
    "        args = _extract_json_from_text(content or \"\")\n",
    "        result = _build_slice_set_from_args(args)\n",
    "        final_html = get_slices(html_content, result)\n",
    "        _write_log(\"final_sliced_html.txt\", final_html)\n",
    "        print(f\"LLM took {dt:.2f}s. Produced {len(result.slices)} slices.\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"LLM call failed: {e}\")\n",
    "        _write_log(\"ollama_response.txt\", f\"ERROR: {e}\")\n",
    "        _write_log(\"parsed_tool_args.txt\", json.dumps({\"slices\": []}, ensure_ascii=False))\n",
    "        return SliceSet(slices=[])\n",
    "# Logging utilities\n",
    "def _resolve_base_dir() -> str:\n",
    "    candidates = [os.getcwd(), os.path.dirname(os.getcwd())]\n",
    "    for c in candidates:\n",
    "        # Prefer a direct prompts folder in the current dir\n",
    "        if os.path.isdir(os.path.join(c, \"prompts\")):\n",
    "            return c\n",
    "    return os.getcwd()\n",
    "\n",
    "BASE_DIR = _resolve_base_dir()\n",
    "LOG_DIR = os.path.join(BASE_DIR, \"logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "def _write_log(filename: str, content: str) -> str:\n",
    "    \"\"\"Write log content to file, overwriting on each run\"\"\"\n",
    "    path = os.path.join(LOG_DIR, filename)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    return path\n",
    "\n",
    "def _append_log(filename: str, content: str) -> str:\n",
    "    \"\"\"Append log content to file\"\"\"\n",
    "    path = os.path.join(LOG_DIR, filename)\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(content + \"\\n\")\n",
    "    return path\n",
    "\n",
    "# Ollama helpers for slicing\n",
    "def _get_ollama_base_url() -> str:\n",
    "    return os.getenv(\"OLLAMA_BASE_URL\", \"http://69.19.137.175:11434\")\n",
    "\n",
    "def _select_model(preferred: List[str] | None = None) -> str:\n",
    "    preferred = preferred or [\"gpt-oss:120b\"]\n",
    "    try:\n",
    "        import ollama\n",
    "        client = ollama.Client(host=_get_ollama_base_url())\n",
    "        try:\n",
    "            ps = client.ps()\n",
    "            running = [m.get('name') or m.get('model') for m in ps.get('models', []) if isinstance(m, dict)]\n",
    "        except Exception:\n",
    "            running = []\n",
    "        try:\n",
    "            listed = client.list()\n",
    "            listed_models = [m.get('name') or m.get('model') for m in listed.get('models', []) if isinstance(m, dict)]\n",
    "        except Exception:\n",
    "            listed_models = []\n",
    "        candidates = [n for n in running + listed_models if isinstance(n, str) and n]\n",
    "        for cand in (preferred or []):\n",
    "            if cand in candidates:\n",
    "                return cand\n",
    "        if candidates:\n",
    "            return candidates[0]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return (preferred or [\"gpt-oss:120b\"])[0]\n",
    "\n",
    "def get_slices(html_content: str, slice_set: SliceSet) -> str:\n",
    "    \"\"\"Extract slices from HTML content based on SliceSet\"\"\"\n",
    "    if not slice_set.slices:\n",
    "        return \"\"\n",
    "    lines = html_content.split('\\n')\n",
    "    chunks: List[str] = []\n",
    "    for sl in slice_set.slices:\n",
    "        a = max(0, sl.first_line)\n",
    "        b = min(len(lines) - 1, sl.last_line)\n",
    "        if a <= b:\n",
    "            chunks.append('\\n'.join(lines[a:b+1]))\n",
    "    return '\\n\\n'.join(chunks)\n",
    "\n",
    "def generate_slices_via_tools(html_content: str) -> SliceSet:\n",
    "    \"\"\"Generate HTML slices using Ollama function calling\"\"\"\n",
    "    \n",
    "    # Truncate HTML for fast testing\n",
    "    max_html_tokens = 16000  # Fast testing limit\n",
    "    estimated_tokens = len(html_content) // 4\n",
    "    \n",
    "    if estimated_tokens > max_html_tokens:\n",
    "        max_chars = max_html_tokens * 4\n",
    "        html_content = html_content[:max_chars]\n",
    "        # End at line boundary\n",
    "        last_newline = html_content.rfind('\\n')\n",
    "        if last_newline > max_chars * 0.9:\n",
    "            html_content = html_content[:last_newline]\n",
    "        print(f\"HTML truncated to {len(html_content)} chars (~{len(html_content)//4} tokens) for fast testing\")\n",
    "    \n",
    "    _write_log(\"raw_html.txt\", html_content)\n",
    "\n",
    "    prompt_tmpl = load_prompt(\"generate_slices\")\n",
    "    schema = SliceSet.model_json_schema()\n",
    "    schema_json = json.dumps(schema, ensure_ascii=False)\n",
    "    prompt = prompt_tmpl.format(html_content=html_content, output_schema=schema_json)\n",
    "    \n",
    "    # Log Ollama prompt and response\n",
    "    _write_log(\"ollama_prompt.txt\", prompt)\n",
    "\n",
    "    tool_schema = schema\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You MUST call the get_slices function with JSON that EXACTLY matches the provided JSON Schema. Use 'first_line' and 'last_line' keys, not 'start' and 'end'. Return NO other text.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    tools = [{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_slices\",\n",
    "            \"description\": \"Return slice ranges to extract relevant HTML lines.\",\n",
    "            \"parameters\": tool_schema\n",
    "        }\n",
    "    }]\n",
    "\n",
    "    import ollama\n",
    "    client = ollama.Client(host=_get_ollama_base_url())\n",
    "    t0 = time.time()\n",
    "    # resp = client.chat(model=_select_model(), messages=messages, tools=tools, options={\"temperature\": 0, \"num_ctx\": 131072})\n",
    "\n",
    "    resp = client.chat(\n",
    "        model=_select_model(),\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        options={\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 16384,\n",
    "            \"num_batch\": 1024,\n",
    "            \"mirostat\": 0,\n",
    "            \"repeat_penalty\": 1.1,\n",
    "            \"seed\": 12345\n",
    "        },\n",
    "        keep_alive=\"0\",              # donâ€™t reuse any history/session\n",
    "    )\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    try:\n",
    "        ollama_response = json.dumps(resp, ensure_ascii=False, indent=2)\n",
    "    except Exception:\n",
    "        ollama_response = str(resp)\n",
    "    \n",
    "    _write_log(\"ollama_response.txt\", f\"OLLAMA RESPONSE ({dt:.2f}s)\\n{ollama_response}\")\n",
    "\n",
    "    # Extract args from response (adapted for generate API)\n",
    "    args: Dict = {}\n",
    "    \n",
    "    # For generate API, the response is in resp.response field\n",
    "    response_text = getattr(resp, 'response', '') or getattr(resp, 'content', '')\n",
    "    if response_text:\n",
    "        try:\n",
    "            # Clean the response text\n",
    "            response_text = response_text.strip()\n",
    "            \n",
    "            # Remove any leading/trailing non-JSON content\n",
    "            start_idx = response_text.find('{')\n",
    "            end_idx = response_text.rfind('}')\n",
    "            \n",
    "            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:\n",
    "                json_text = response_text[start_idx:end_idx+1]\n",
    "                args = json.loads(json_text)\n",
    "                print(f\"Parsed JSON from generate response: {len(args)} keys\")\n",
    "                print(f\"Slices found in response: {len(args.get('slices', []))}\")\n",
    "            else:\n",
    "                print(f\"No valid JSON found in response: {response_text[:200]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing JSON from generate response: {e}\")\n",
    "            print(f\"Response text: {response_text[:200]}...\")\n",
    "    \n",
    "    # Fallback to original function calling logic for compatibility\n",
    "    if not args and hasattr(resp, 'message') and hasattr(resp.message, 'tool_calls') and resp.message.tool_calls:\n",
    "        raw_args = resp.message.tool_calls[0].function.arguments\n",
    "        if isinstance(raw_args, str):\n",
    "            try:\n",
    "                args = json.loads(raw_args)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing tool call arguments: {e}\")\n",
    "                args = {}\n",
    "        elif isinstance(raw_args, dict):\n",
    "            args = raw_args\n",
    "        print(f\"Tool call arguments extracted: {len(args)} keys\")\n",
    "        print(f\"Slices found in args: {len(args.get('slices', []))}\")\n",
    "    elif hasattr(resp, 'message') and hasattr(resp.message, 'content') and resp.message.content:\n",
    "        # Fallback: try to parse JSON from message content\n",
    "        try:\n",
    "            content = resp.message.content.strip()\n",
    "            if content.startswith('{') and content.endswith('}'):\n",
    "                args = json.loads(content)\n",
    "                print(f\"Parsed JSON from message content: {len(args)} keys\")\n",
    "                print(f\"Slices found in content: {len(args.get('slices', []))}\")\n",
    "            else:\n",
    "                print(f\"Message content is not JSON: {content[:100]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing JSON from message content: {e}\")\n",
    "     \n",
    "     # Additional fallback: try to extract from any response field that might contain JSON\n",
    "    if not args:\n",
    "        # Check if there's JSON in the main response field\n",
    "        response_content = getattr(resp, 'response', '') or getattr(resp, 'content', '')\n",
    "        if response_content:\n",
    "            try:\n",
    "                if response_content.strip().startswith('{') and response_content.strip().endswith('}'):\n",
    "                    args = json.loads(response_content.strip())\n",
    "                    print(f\"Parsed JSON from response field: {len(args)} keys\")\n",
    "                else:\n",
    "                    # Try to find JSON within the response\n",
    "                    import re\n",
    "                    json_match = re.search(r'\\{[^{}]*\"slices\"[^{}]*\\}', response_content)\n",
    "                    if json_match:\n",
    "                        args = json.loads(json_match.group())\n",
    "                        print(f\"Extracted JSON from response text: {len(args)} keys\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing JSON from response field: {e}\")\n",
    "    \n",
    "    if not args:\n",
    "        print(\"No tool calls found in response - model may be generating text instead of calling function\")\n",
    "\n",
    "    # Build SliceSet (handle dict or [start, end])\n",
    "    parsed_preview = []\n",
    "    slices: List[Slice] = []\n",
    "    for sl in (args.get('slices') or []):\n",
    "        a = None\n",
    "        b = None\n",
    "        if isinstance(sl, dict):\n",
    "            # Handle multiple formats: start/end, first_line/last_line, or first/last\n",
    "            if 'start' in sl and 'end' in sl:\n",
    "                a = sl.get('start', None)\n",
    "                b = sl.get('end', None)\n",
    "            elif 'first_line' in sl and 'last_line' in sl:\n",
    "                a = sl.get('first_line', None)\n",
    "                b = sl.get('last_line', None)\n",
    "            elif 'first' in sl and 'last' in sl:\n",
    "                a = sl.get('first', None)\n",
    "                b = sl.get('last', None)\n",
    "        elif isinstance(sl, list) and len(sl) >= 2:\n",
    "            a, b = sl[0], sl[1]\n",
    "        try:\n",
    "            a = int(a)\n",
    "            b = int(b)\n",
    "        except Exception:\n",
    "            continue\n",
    "        a = max(0, a)\n",
    "        b = max(a, b)\n",
    "        slices.append(Slice(first_line=a, last_line=b))\n",
    "        parsed_preview.append([a, b])\n",
    "\n",
    "    _write_log(\"parsed_tool_args.txt\", json.dumps({\"slices\": parsed_preview}, ensure_ascii=False))\n",
    "\n",
    "    result = SliceSet(slices=slices)\n",
    "    final_html = get_slices(html_content, result)\n",
    "    _write_log(\"final_sliced_html.txt\", final_html)\n",
    "\n",
    "    print(f\"LLM + tools took {dt:.2f}s. Produced {len(result.slices)} slices.\")\n",
    "    return result\n",
    "\n",
    "# search_starters = [\"Digital Marketing Firm\", \"Tax Consultancy Firm\", \"Cybersecurity Firm\", \"Legal Services Provider\", \"Healthcare Consultancy\"]\n",
    "\n",
    "# search_starters = [\"Company formation service UAE\", \"Business formation service MENA\", \"Business registration services Gulf\", \"Business setup services Dubai\", \"Investment migration services UAE\", \"Second passport services\", \"Citizenship By investment\", \"economic passport services\", \"Golden Visa services\", \"Tax services UAE\", \"Tax Consultancy Firm Dubai\", \"Tax Calculation Services Dubai\"]\n",
    "\n",
    "class ResultRelevance(BaseModel):\n",
    "    explanation: str = Field(description=\"Be extremely brief and concise with your explanation.\")\n",
    "    link: str\n",
    "\n",
    "class RelevanceCheckOutput(BaseModel):\n",
    "    most_relevant_result: Optional[ResultRelevance]\n",
    "\n",
    "class SqlCommand(BaseModel):\n",
    "    command: str\n",
    "\n",
    "\n",
    "class ServiceProviderMemberDetails(BaseModel):\n",
    "    name: str = Field(description=\"Very important. Find the name of the staff member from the CLEAN HTML content.\")\n",
    "    role_description: str = Field(description=\"Find the role description of the staff member from the CLEAN HTML content.\")\n",
    "    telephone: str = Field(description=\"Find the telephone number of the staff member from the CLEAN HTML content.\")\n",
    "    mobile: str = Field(description=\"Find the mobile number of the staff member from the CLEAN HTML content.\")\n",
    "    email: str = Field(description=\"Find the email address of the staff member from the CLEAN HTML content.\")\n",
    "    linkedin: Optional[str]\n",
    "    facebook: Optional[str]\n",
    "    instagram: Optional[str]\n",
    "    twitter: Optional[str]\n",
    "    additional_info: Optional[str]\n",
    "\n",
    "class ServiceProviderOutput(BaseModel):\n",
    "    service_title: Optional[str] = Field(description=\"Produce a suitable title for the service.\")\n",
    "    service_description: Optional[str] = Field(description=\"Do your due diligence on summarising the service description based on a holistic consideration of the scattered relevant service info throughout the entirety of the given CLEAN HTML content. Use a string, not a number. Null if unavailable.\")\n",
    "    rating_score: Optional[float] = Field(description=\"Must adhere to a rating standard out of 5. Must be a numeric rating score of type float. Use a number, not a string. Null if unavailable.\")\n",
    "    rating_description: Optional[str] = Field(description=\"Do your due diligence on summarising the rating description based on a holistic consideration of the scattered relevant rating info throughout the entirety of the given CLEAN HTML content. Use a string, not a number. Null if unavailable.\")\n",
    "    pricing: Optional[str]\n",
    "    service_tags: List[str] = Field(description=\"Do your due diligence on summarising the service tags based on a holistic consideration of the scattered relevant service info throughout the entirety of the given CLEAN HTML content. Null if unavailable.\")\n",
    "    provider_type: ProviderType = Field(\"Company\", description=\"ProviderType enum. Should be Company if the provider is a company or Agent if the provider is an agent.\")\n",
    "    # provider_type: ProviderType = Field(description=\"ProviderType enum. Should be Company if the provider is a company or Agent if the provider is an agent.\")\n",
    "    country: Optional[Country] = Field(description=\"Country enum.\")\n",
    "    name: Optional[str] = Field(description=\"Very important. Find the name of the company from the CLEAN HTML content.\")\n",
    "    vision: Optional[str] = Field(description=\"A brief summary of the company/firm's vision statement from the CLEAN HTML content.\")\n",
    "    logo: Optional[str] = Field(description=\"Mandatory. Find the absolute URL for the company logo from the CLEAN HTML content.\")\n",
    "    website: Optional[str] = Field(description=\"Very important. Must find the website link of the company from the HTML content or infer it from the base website link address. Null if unavailable.\")\n",
    "    linkedin: Optional[str] = Field(description=\"Null if unavailable.\")\n",
    "    facebook: Optional[str] = Field(description=\"Null if unavailable.\")\n",
    "    instagram: Optional[str] = Field(description=\"Null if unavailable.\")\n",
    "    telephones: List[str] = Field([], description=\"Mandatory. Must find at least one telephone number. Field must be a list of telephone numbers or empty list.\")\n",
    "    mobiles: List[str] = Field([], description=\"Mandatory. Must find at least one mobile number. Field must be a list of mobile numbers or empty list.\")\n",
    "    emails: List[str] = Field(\n",
    "        [], description=\"Mandatory. Must find at least one email address of the company from the HTML content. Field must be a list of email addresses or empty list.\"\n",
    "    )\n",
    "    # emails: List[str] = Field(description=\"Very very important. Must find at least one email address of the company from the HTML content. Output must be a list of email addresses.\")\n",
    "    office_locations: Optional[str] = Field(description=\"Detailed address locations of the company offices eg. 123 Main St, City, Country, or description if address is not available. Null if unavailable.\")\n",
    "    key_individuals: Optional[str] = Field(description=\"Names and role descriptions of key people of the company. Null if unavailable.\")\n",
    "    representatives: Optional[str] = Field(description=\"Names and roles and contact details of representatives of the company. Null if unavailable.\")\n",
    "    # service_provider_member_details: Optional[List[ServiceProviderMemberDetails]]\n",
    "\n",
    "class SearchTermOutput(BaseModel):\n",
    "    search_term: str\n",
    "\n",
    "class AboutUsOutput(BaseModel):\n",
    "    about_us_link: Optional[List[str]]\n",
    "\n",
    "def call_llm(\n",
    "    model = \"claude-3-5-sonnet-20241022\",\n",
    "    API_key = None,\n",
    "    prompt_template = None,\n",
    "    input_dict = None,\n",
    "    structured_output = None,\n",
    "    temperature = 0,\n",
    "    prompt_name = None,\n",
    "):\n",
    "    if API_key is None:\n",
    "        API_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    if not API_key:\n",
    "        raise RuntimeError(\"Missing ANTHROPIC_API_KEY. Set it or pass API_key.\")\n",
    "    if prompt_template is None:\n",
    "        raise ValueError(\"Prompt template is required\")\n",
    "    if input_dict is None:\n",
    "        raise ValueError(\"Input dictionary is required\")\n",
    "    if structured_output is None:\n",
    "        raise ValueError(\"Structured output is required\")\n",
    "    \n",
    "    # Log Anthropic prompt\n",
    "    if prompt_name:\n",
    "        try:\n",
    "            # Format the prompt to see what will be sent\n",
    "            formatted_messages = prompt_template.format_messages(**input_dict)\n",
    "            prompt_content = \"\\n\".join([f\"{msg.type.upper()}: {msg.content}\" for msg in formatted_messages])\n",
    "            _write_log(f\"anthropic_{prompt_name}_prompt.txt\", \n",
    "                      f\"MODEL: {model}\\nTEMPERATURE: {temperature}\\nSTRUCTURED_OUTPUT: {structured_output.__name__}\\nINPUT_DICT: {json.dumps(input_dict, ensure_ascii=False, indent=2)}\\n\\n--- FORMATTED PROMPT ---\\n{prompt_content}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error logging Anthropic prompt: {e}\")\n",
    "    \n",
    "    llm = ChatAnthropic(\n",
    "        model=model,\n",
    "        anthropic_api_key=API_key, temperature=temperature).with_structured_output(structured_output)\n",
    "    llm_chain = prompt_template | llm\n",
    "    llm_chain = llm_chain.with_retry(\n",
    "        retry_if_exception_type=(OverloadedError,),\n",
    "        wait_exponential_jitter=True,    \n",
    "        stop_after_attempt=6\n",
    "    )\n",
    "    \n",
    "    t0 = time.time()\n",
    "    result = llm_chain.invoke(input_dict)\n",
    "    dt = time.time() - t0\n",
    "    \n",
    "    # Log Anthropic response\n",
    "    if prompt_name:\n",
    "        try:\n",
    "            response_content = json.dumps(result.dict() if hasattr(result, 'dict') else result.__dict__, ensure_ascii=False, indent=2)\n",
    "            _append_log(f\"anthropic_{prompt_name}_prompt.txt\", \n",
    "                       f\"\\n--- ANTHROPIC RESPONSE ({dt:.2f}s) ---\\n{response_content}\")\n",
    "        except Exception as e:\n",
    "            _append_log(f\"anthropic_{prompt_name}_prompt.txt\", \n",
    "                       f\"\\n--- ANTHROPIC RESPONSE ({dt:.2f}s) ---\\n{str(result)}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def search_serper(search_query):\n",
    "    url = \"https://google.serper.dev/search\"\n",
    "    \n",
    "    payload = json.dumps({\n",
    "        \"q\": search_query,\n",
    "        \"gl\": \"ae\", \n",
    "        \"num\": 10,\n",
    "        # \"tbs\": \"qdr:w\"\n",
    "    })\n",
    "\n",
    "    headers = {\n",
    "        'X-API-KEY': os.environ[\"SERPER_API_KEY\"],\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload, timeout=(3,4))\n",
    "    results = json.loads(response.text)\n",
    "    results_list = results['organic']\n",
    "\n",
    "    all_results = []\n",
    "    for id, result in enumerate(results_list, 1):\n",
    "        result_dict = {\n",
    "            'title': result['title'],\n",
    "            'link': result['link'],\n",
    "            'snippet': result['snippet'],\n",
    "            'search_term': search_query,\n",
    "            'id': id\n",
    "        }\n",
    "        all_results.append(result_dict)\n",
    "    return all_results\n",
    "\n",
    "def load_prompt(prompt_name):\n",
    "    with open(f\"prompts/{prompt_name}.md\", \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "\n",
    "def check_search_relevance(search_term: str, search_results: Dict[str, Any]) -> RelevanceCheckOutput:\n",
    "    prompt = load_prompt(\"relevance_check_one_link\")\n",
    "    \n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", prompt),\n",
    "        (\"human\", f\"Check relevance of search results to the given search term (but most importantly make sure it is the actual company's official website, not a search facilitator website or other service of that sort): {search_term}\")\n",
    "    ])\n",
    "    \n",
    "    return call_llm(prompt_template = prompt_template, input_dict = {\"search_term\": search_term, 'search_results': search_results}, structured_output = RelevanceCheckOutput, temperature=0, prompt_name=\"relevance_check_one_link\")\n",
    "\n",
    "\n",
    "def convert_html_to_markdown(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Headers\n",
    "    for h in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "        level = int(h.name[1])\n",
    "        h.replace_with('#' * level + ' ' + h.get_text() + '\\n\\n')\n",
    "    \n",
    "    # Links\n",
    "    for a in soup.find_all('a'):\n",
    "        href = a.get('href', '')\n",
    "        text = a.get_text()\n",
    "        if href and text:\n",
    "            a.replace_with(f'[{text}]({href})')\n",
    "    \n",
    "    # Bold\n",
    "    for b in soup.find_all(['b', 'strong']):\n",
    "        b.replace_with(f'**{b.get_text()}**')\n",
    "    \n",
    "    # Italic\n",
    "    for i in soup.find_all(['i', 'em']):\n",
    "        i.replace_with(f'*{i.get_text()}*')\n",
    "    \n",
    "    # Lists\n",
    "    for ul in soup.find_all('ul'):\n",
    "        for li in ul.find_all('li'):\n",
    "            li.replace_with(f'- {li.get_text()}\\n')\n",
    "    \n",
    "    for ol in soup.find_all('ol'):\n",
    "        for i, li in enumerate(ol.find_all('li'), 1):\n",
    "            li.replace_with(f'{i}. {li.get_text()}\\n')\n",
    "    \n",
    "    # Get text and clean up\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Remove excess whitespace/newlines\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def scrape_and_save_markdown(relevant_results):\n",
    "    markdown_contents = []\n",
    "    for result in relevant_results:\n",
    "        if 'link' in result:\n",
    "            payload = {\n",
    "                \"api_key\": os.environ[\"SCRAPING_API_KEY\"], \n",
    "                \"url\": result['link'],\n",
    "                \"render_js\": \"true\"\n",
    "            }\n",
    "\n",
    "            response = requests.get(\"https://scraping.narf.ai/api/v1/\", params=payload)\n",
    "            if response.status_code == 200:\n",
    "                markdown_content = convert_html_to_markdown(response.content.decode())\n",
    "                \n",
    "                markdown_contents.append({\n",
    "                    'url': result['link'],\n",
    "                    'markdown': markdown_content,\n",
    "                    'title': result.get('title', ''),\n",
    "                    'id': result.get('id', '')\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Failed to fetch {result['link']}: Status code {response.status_code}\")\n",
    "\n",
    "    print(f\"Successfully downloaded and saved {len(markdown_contents)} pages as markdown\")\n",
    "    return markdown_contents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scrapper import HtmlScraper\n",
    "import tiktoken\n",
    "import string\n",
    "import secrets\n",
    "\n",
    "import os\n",
    "import django\n",
    "import sys\n",
    "from asgiref.sync import sync_to_async\n",
    "from urllib.parse import urlparse\n",
    "from django.core.validators import validate_email, URLValidator\n",
    "from django.core.exceptions import ValidationError\n",
    "\n",
    "sys.path.insert(0, '../growbal_django')\n",
    "\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"growbal.settings\")\n",
    "\n",
    "django.setup()\n",
    "\n",
    "from services.models import Service\n",
    "from services.serializers import ServiceSerializer\n",
    "from accounts.serializers import ServiceProviderProfileSerializer\n",
    "from accounts.models import CustomUser, ServiceProviderProfile\n",
    "from asgiref.sync import sync_to_async\n",
    "# from django.db import transaction\n",
    "from scraper.models import Scrape          # adjust import path as needed\n",
    "from scraper.serializers import ScrapeSerializer\n",
    "# from django.core.files import File\n",
    "\n",
    "def validate_emails(email_list):\n",
    "    valid_emails = []\n",
    "    for email in email_list:\n",
    "        try:\n",
    "            validate_email(email)\n",
    "            valid_emails.append(email)\n",
    "        except ValidationError:\n",
    "            error_message = f\"Error validating email {email}\"\n",
    "            print(error_message)\n",
    "            with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                log_file.write(error_message + \"\\n\")\n",
    "    return valid_emails\n",
    "\n",
    "\n",
    "_url_validator = URLValidator()\n",
    "def validate_url(raw_url: str | None) -> bool:\n",
    "    if not raw_url:\n",
    "        return False\n",
    "\n",
    "    raw_url = raw_url.strip()\n",
    "    if not urlparse(raw_url).scheme:\n",
    "        raw_url = \"http://\" + raw_url\n",
    "\n",
    "    try:\n",
    "        _url_validator(raw_url)\n",
    "        return True\n",
    "    except ValidationError:\n",
    "        return False\n",
    "\n",
    "\n",
    "@sync_to_async\n",
    "def get_or_create_user(name=None, email=None, username=None, password=None):\n",
    "    user, created = CustomUser.objects.get_or_create_user(\n",
    "        name=name,\n",
    "        email=email,\n",
    "        username=username,\n",
    "        password=password\n",
    "    )\n",
    "    return user, created\n",
    "\n",
    "@sync_to_async\n",
    "def create_service(data):\n",
    "    serializer = ServiceSerializer(data=data)\n",
    "    if serializer.is_valid():\n",
    "        service = serializer.save()\n",
    "        return service\n",
    "    else:\n",
    "        error_message = f\"Error validating service data {serializer.errors}\"\n",
    "        print(error_message)\n",
    "        with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "            log_file.write(error_message + \"\\n\")\n",
    "\n",
    "@sync_to_async\n",
    "def create_service_provider_profile(data):\n",
    "    serializer = ServiceProviderProfileSerializer(data=data)\n",
    "    if serializer.is_valid():\n",
    "        profile = serializer.save()\n",
    "        return profile\n",
    "    else:\n",
    "        error_message = f\"Error validating service provider profile data {serializer.errors}\"\n",
    "        print(error_message)\n",
    "        with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "            log_file.write(error_message + \"\\n\")\n",
    "        # raise ValueError(serializer.errors)\n",
    "\n",
    "@sync_to_async\n",
    "def create_scrape(data):\n",
    "    serializer = ScrapeSerializer(data=data)\n",
    "    if serializer.is_valid():\n",
    "        scrape = serializer.save()\n",
    "        return scrape\n",
    "    else:\n",
    "        error_message = f\"Error validating scrape data {serializer.errors}\"\n",
    "        print(error_message)\n",
    "        with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "            log_file.write(error_message + \"\\n\")\n",
    "        # raise ValueError(serializer.errors)\n",
    "\n",
    "@sync_to_async\n",
    "def check_similar_scrapes(base_url):\n",
    "    return Scrape.check_similar_base_url(base_url=base_url)\n",
    "\n",
    "def generate_password(length=8):\n",
    "    characters = string.ascii_letters + string.digits + string.punctuation\n",
    "    password = ''.join(secrets.choice(characters) for _ in range(length))\n",
    "    return password\n",
    "\n",
    "# def count_tokens(text, model=\"gpt-4\"):\n",
    "#     encoding = tiktoken.encoding_for_model(model)\n",
    "#     tokens = encoding.encode(text)\n",
    "#     return len(tokens)\n",
    "\n",
    "anthropic_client = anthropic.Anthropic()\n",
    "\n",
    "def count_tokens(text, model=\"claude-3-5-sonnet-20241022\", system_prompt=\"\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": text}]\n",
    "    \n",
    "    response = anthropic_client.messages.count_tokens(\n",
    "        model=model,\n",
    "        system=system_prompt,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.input_tokens\n",
    "\n",
    "scraper = HtmlScraper(scraping_api_key=os.environ[\"SCRAPING_API_KEY\"])\n",
    "\n",
    "async def process_relevant_links(relevant_link, row_dict):\n",
    "    if await check_similar_scrapes(relevant_link.link):\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Processing {relevant_link.link}\")\n",
    "        \n",
    "        nav_html_list = []\n",
    "        try:\n",
    "                num_tokens_allowed = 38000\n",
    "                max_page_tokens = 13000\n",
    "                # min_page_num = 7\n",
    "                i = 1\n",
    "                clean_html_sum_links = []\n",
    "                clean_html_sum = \"\"\n",
    "\n",
    "                try:\n",
    "                    print(\"----------BASE PAGE----------\")\n",
    "                    print(relevant_link.link)\n",
    "                    nav_html_content_base = scraper.scrape_html_base(relevant_link.link)\n",
    "                    \n",
    "                    # Log raw scraped HTML for base page\n",
    "                    if nav_html_content_base:\n",
    "                        _write_log(\"base_raw_scraped_base_html.txt\", nav_html_content_base)\n",
    "                    \n",
    "                    clean_text, clean_html = scraper.clean_html_content(nav_html_content_base)\n",
    "                    \n",
    "                    # scraper.save_html(clean_html, relevant_link.link)\n",
    "                    # if clean_html and (token_count := count_tokens(clean_html)) < max_page_tokens/2:\n",
    "                    print(f\"clean_html: {count_tokens(clean_html)} tokens\")\n",
    "                    if clean_html and (token_count := count_tokens(clean_html)) < 65536:\n",
    "                        print(f\"clean_html: {token_count} tokens\")\n",
    "                        print(\"approved\")\n",
    "                        _write_log(\"base_clean_html_before_slicing.txt\", clean_html)\n",
    "                        \n",
    "                        # Apply HTML slicing pipeline\n",
    "                        slice_set = generate_slices_simple(clean_html)\n",
    "                        sliced_html = get_slices(clean_html, slice_set)\n",
    "                        \n",
    "                        # Log clean HTML after slicing\n",
    "                        _write_log(\"base_clean_html_after_slicing.txt\", sliced_html)\n",
    "                        \n",
    "                        clean_html_sum += f\"CLEAN HTML PAGE {i}: ({relevant_link.link})\\n\\n\"\n",
    "                        clean_html_sum += sliced_html + \"\\n\\n\"\n",
    "                        clean_html_sum_links.append(relevant_link.link)\n",
    "                    elif clean_text and (token_count := count_tokens(clean_text)) < max_page_tokens:\n",
    "                        print(f\"clean_text: {token_count} tokens\")\n",
    "                        print(\"approved\")\n",
    "                        clean_html_sum += f\"ONLY TEXT FROM HTML PAGE {i}: ({relevant_link.link})\\n\\n\"\n",
    "                        clean_html_sum += clean_text + \"\\n\\n\"\n",
    "                        clean_html_sum_links.append(relevant_link.link)\n",
    "                    if nav_html_content_base:\n",
    "                        nav_html_list.append(nav_html_content_base)\n",
    "                except Exception as e:\n",
    "                    error_message = f\"Error processing base page: {e}\"\n",
    "                    print(error_message)\n",
    "                    with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                        log_file.write(error_message + \"\\n\")\n",
    "\n",
    "                nav_links = []\n",
    "                if len(nav_html_list) == 0 or not nav_html_list[0]: \n",
    "                    return\n",
    "                for html_content in nav_html_list:\n",
    "                    try:\n",
    "                        nav_links.extend(scraper.get_nav_links(html_content))\n",
    "                    except Exception as e:\n",
    "                        error_message = f\"Error processing nav links: {e}\"\n",
    "                        print(error_message)\n",
    "                        with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                            log_file.write(error_message + \"\\n\")\n",
    "                print(\"----------NAV LINKS----------\")\n",
    "                print(nav_links)\n",
    "                nav_links_str = \"\\n\".join([f\"{title}: {link}\" for title, link, _ in nav_links])\n",
    "\n",
    "                prompt = load_prompt(\"generate_about_us_link\")\n",
    "                prompt_template = ChatPromptTemplate.from_messages([\n",
    "                    (\"system\", prompt),\n",
    "                    (\"human\", f'Filter \"about us\" page, contact us page, services page, team page, or any similar page.')\n",
    "                ])\n",
    "                \n",
    "\n",
    "                result = call_llm(prompt_template = prompt_template, input_dict = {\"nav_links\": nav_links_str, \"base_url\": relevant_link.link}, structured_output = AboutUsOutput, temperature=0, prompt_name=\"generate_about_us_link\")\n",
    "                print(len(result.about_us_link))\n",
    "                print(result.about_us_link)\n",
    "                print(\"\\n\")\n",
    "                for i, link in enumerate(result.about_us_link):\n",
    "                    if link in clean_html_sum_links:\n",
    "                        continue\n",
    "                    if clean_html_sum and count_tokens(clean_html_sum) >= num_tokens_allowed:\n",
    "                        continue\n",
    "                    try:\n",
    "                        html_content = scraper.scrape_html_base(link)\n",
    "                        \n",
    "                        # Log raw scraped HTML for additional pages\n",
    "                        if html_content:\n",
    "                            _write_log(\"raw_scraped_additional_html.txt\", html_content)\n",
    "                        \n",
    "                        clean_text, clean_html = scraper.clean_html_content(html_content)\n",
    "                        \n",
    "                        if clean_html: print(f\"count_tokens(clean_html) {i} before approval: {count_tokens(clean_html)}\")\n",
    "                        else: print(f\"not clean_text {i} before approval: {clean_html}\")\n",
    "\n",
    "                        # if clean_html and (count_tokens(clean_html) < max_page_tokens and count_tokens(clean_html) < num_tokens_allowed - count_tokens(clean_html_sum) if clean_html_sum else 0):\n",
    "                        if clean_html and (count_tokens(clean_html) < 65536):\n",
    "                            _write_log(\"clean_html_before_slicing_additional.txt\", clean_html)\n",
    "                            \n",
    "                            # Apply HTML slicing pipeline\n",
    "                            slice_set = generate_slices_simple(clean_html)\n",
    "                            sliced_html = get_slices(clean_html, slice_set)\n",
    "                            \n",
    "                            # Log clean HTML after slicing for additional pages\n",
    "                            _write_log(\"clean_html_after_slicing_additional.txt\", sliced_html)\n",
    "                            \n",
    "                            clean_html_sum += f\"CLEAN HTML PAGE {i}: ({link})\\n\\n\"\n",
    "                            clean_html_sum += sliced_html + \"\\n\\n\"\n",
    "                            clean_html_sum_links.append(link)\n",
    "                            print(f\"{i} approved {link}\")\n",
    "                        elif (token_count := count_tokens(clean_html_sum)) < num_tokens_allowed:\n",
    "                            print(token_count)\n",
    "                            clean_html_sum += f\"ONLY TEXT FROM HTML PAGE {i}: ({link})\\n\\n\"\n",
    "                            clean_html_sum += clean_text + \"\\n\\n\"\n",
    "                            clean_html_sum_links.append(link)\n",
    "                            print(f\"{i} approved {link}\")\n",
    "                        else:\n",
    "                            break\n",
    "                        i += 1\n",
    "                    except Exception as e:\n",
    "                        error_message = f\"Error processing {link}: {e}\"\n",
    "                        print(error_message)\n",
    "                        with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                            log_file.write(error_message + \"\\n\")\n",
    "                \n",
    "                # print(\"##########PROPRIETARY DATA RECORD OF THE ORGANIZATION##########\\n\")\n",
    "                # print(str(row_dict))\n",
    "                # with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                #     log_file.write(\"PROPRIETARY DATA RECORD OF THE ORGANIZATION\\n\\n\" + str(row_dict) + \"\\n\\n\")\n",
    "                # html_appendex = \"PROPRIETARY DATA RECORD OF THE ORGANIZATION\\n\\n\" + str(row_dict)\n",
    "                clean_html_sum = clean_html_sum[:int((num_tokens_allowed*4)/2)]\n",
    "                if clean_html_sum != \"\":\n",
    "                    # Log the final clean HTML sum before field generation\n",
    "                    _write_log(\"final_clean_html_sum.txt\", clean_html_sum)\n",
    "                    \n",
    "                    prompt = load_prompt(\"generate_fields_with_email\")\n",
    "                    prompt_template = ChatPromptTemplate.from_messages([\n",
    "                        (\"system\", prompt),\n",
    "                        (\"human\", f\"Generate fields from the given HTML content.\")\n",
    "                    ])\n",
    "                    \n",
    "                    result = call_llm(prompt_template = prompt_template, input_dict = {\"html_content\": clean_html_sum, \"proprietary_data\": str(row_dict)}, structured_output = ServiceProviderOutput, temperature=0, prompt_name=\"generate_fields_with_email\")\n",
    "                    if result.emails == None:\n",
    "                        print(\"result.email is Null\")\n",
    "                        result.emails = []\n",
    "                    if result.telephones == None:\n",
    "                        print(\"result.telephones is Null\")\n",
    "                        result.telephones = []\n",
    "                    if result.mobiles == None:\n",
    "                        print(\"result.mobiles is Null\")\n",
    "                        result.mobiles = []\n",
    "                    if result.logo:\n",
    "                        try:\n",
    "                            result.logo = scraper.save_logo(result.logo)\n",
    "                        except Exception as e:\n",
    "                            error_message = f\"Error saving logo: {e}\"\n",
    "                            print(error_message)\n",
    "                            with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                                log_file.write(error_message + \"\\n\")\n",
    "                            result.logo = None\n",
    "\n",
    "                result.emails = validate_emails(result.emails)\n",
    "                if hasattr(result, 'name') and result.name and result.name != \"\":\n",
    "                    if hasattr(result, 'name') and result.name and result.name != \"\":\n",
    "                        user = None\n",
    "                        if validate_url(result.website):\n",
    "                            result.website = result.website.strip()\n",
    "                            if not urlparse(result.website).scheme:\n",
    "                                result.website = \"http://\" + result.website\n",
    "                            try:\n",
    "                                profile = await ServiceProviderProfile.objects.aget(website=result.website)\n",
    "                                if profile:\n",
    "                                    log_message = f\"Profile already exists for this website: {result.website}\"\n",
    "                                    print(log_message)\n",
    "                                    with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                                        log_file.write(log_message + \"\\n\")\n",
    "                                    user = profile.user\n",
    "                            except Exception as e:\n",
    "                                profile = None\n",
    "                                error_message = f\"Error getting profile from website: {e}\"\n",
    "                                print(error_message)\n",
    "                                with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                                    log_file.write(error_message + \"\\n\")\n",
    "                        else:\n",
    "                            result.website = None\n",
    "                        \n",
    "                        if not user and hasattr(result, 'emails') and result.emails and result.emails[0] != \"\":\n",
    "                            print(\"creating user with name and email\")\n",
    "                            user_email = result.emails[0]\n",
    "                            user_password = generate_password(8)\n",
    "                            user, created = await get_or_create_user(name=result.name, email=user_email, username=user_email, password=user_password)\n",
    "                            print(\"created user with email and name\")\n",
    "                        elif not user:\n",
    "                            user_password = generate_password(8)\n",
    "                            user, created = await get_or_create_user(name=result.name, password=user_password)\n",
    "                            print(\"created user with name only\")                        \n",
    "\n",
    "                        if user:\n",
    "                            # if not result.emails: result.emails = []\n",
    "                            try:\n",
    "                                profile = await ServiceProviderProfile.objects.aget(user_id=user.id)\n",
    "                            except ServiceProviderProfile.DoesNotExist:\n",
    "                                profile = None\n",
    "                                error_message = f\"Error profile does not exist!\"\n",
    "                                print(error_message)\n",
    "                                with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                                    log_file.write(error_message + \"\\n\")\n",
    "                            except Exception as e:\n",
    "                                profile = None\n",
    "                                error_message = f\"Error getting profile: {e}\"\n",
    "                                print(error_message)\n",
    "                                with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                                    log_file.write(error_message + \"\\n\")\n",
    "                            \n",
    "                            if not created and profile:\n",
    "                                data = {\n",
    "                                    \"provider_type\": result.provider_type[:50] if result.provider_type else profile.provider_type,\n",
    "                                    \"country\": result.country[:100] if result.country else profile.country,\n",
    "                                    \"session_status\": profile.session_status,\n",
    "                                    \"name\": result.name[:255] if result.name else profile.name,\n",
    "                                    \"vision\": result.vision or profile.vision,\n",
    "                                    \"website\": result.website if validate_url(result.website) else profile.website,\n",
    "                                    \"logo\": result.logo or profile.logo,\n",
    "                                    \"linkedin\": result.linkedin if validate_url(result.linkedin) else profile.linkedin,\n",
    "                                    \"facebook\": result.facebook if validate_url(result.facebook) else profile.facebook,\n",
    "                                    \"instagram\": result.instagram if validate_url(result.instagram) else profile.instagram,\n",
    "                                    \"telephones\": [x[:30] for x in result.telephones] if result.telephones else profile.telephones,\n",
    "                                    \"mobiles\": [x[:30] for x in result.mobiles] if result.mobiles else profile.mobiles,\n",
    "                                    \"emails\": result.emails if result.emails else profile.emails,\n",
    "                                    \"office_locations\": result.office_locations or profile.office_locations,\n",
    "                                    \"key_individuals\": result.key_individuals or profile.key_individuals,\n",
    "                                    \"representatives\": result.representatives or profile.representatives\n",
    "                                }\n",
    "                                saved_profile, created_profile = await ServiceProviderProfile.objects.aupdate_or_create(\n",
    "                                    user_id=user.id,             # lookup part â€“ must stay unique\n",
    "                                    defaults=data            # values to update/insert\n",
    "                                )   \n",
    "                                print(f\"Profile updated: {saved_profile.name}\")\n",
    "                            else:\n",
    "                                data = {\n",
    "                                    \"user\": user.username,\n",
    "                                    \"provider_type\": result.provider_type[:50] if result.provider_type else None,\n",
    "                                    \"country\": result.country[:100] if result.country else None,\n",
    "                                    \"session_status\": \"inactive\",\n",
    "                                    \"name\": result.name[:255] if result.name else None,\n",
    "                                    \"vision\": result.vision,\n",
    "                                    \"website\": result.website if validate_url(result.website) else None,\n",
    "                                    \"logo\": result.logo,\n",
    "                                    \"linkedin\": result.linkedin if validate_url(result.linkedin) else None,\n",
    "                                    \"facebook\": result.facebook if validate_url(result.facebook) else None,\n",
    "                                    \"instagram\": result.instagram if validate_url(result.instagram) else None,\n",
    "                                    \"telephones\": [x[:30] for x in result.telephones] if result.telephones else [],\n",
    "                                    \"mobiles\": [x[:30] for x in result.mobiles] if result.mobiles else [],\n",
    "                                    \"emails\": result.emails,\n",
    "                                    \"office_locations\": result.office_locations,\n",
    "                                    \"key_individuals\": result.key_individuals,\n",
    "                                    \"representatives\": result.representatives\n",
    "                                }\n",
    "                                saved_profile = await create_service_provider_profile(data)\n",
    "                                print(f\"Profile created: {saved_profile.name}\")\n",
    "\n",
    "                            service_data_json = {\n",
    "                                \"profile\": saved_profile.id,\n",
    "                                \"service_title\": result.service_title[:255] if result.service_title else None,\n",
    "                                \"service_description\": result.service_description,\n",
    "                                \"service_tags\": result.service_tags,\n",
    "                                \"rating_score\": result.rating_score,\n",
    "                                \"rating_description\": result.rating_description,\n",
    "                                \"pricing\": result.pricing\n",
    "                            }\n",
    "                            # Direct async call (works in Jupyter/IPython):\n",
    "                            saved_service = await create_service(service_data_json)\n",
    "                            print(f\"Service created: {saved_service.service_title}\")\n",
    "                            scrape_data = {\n",
    "                                \"base_url\": relevant_link.link,\n",
    "                                \"provider\": saved_profile.id,\n",
    "                                \"service\": saved_service.id,\n",
    "                                \"cleaned_html\": clean_html_sum\n",
    "                            }\n",
    "                            saved_scrape = await create_scrape(scrape_data)\n",
    "                            print(f\"Scrape created: {saved_scrape.base_url}\")\n",
    "        except Exception as e:\n",
    "            # raise ValueError(e)\n",
    "            error_message = f\"Error processing {relevant_link.link}: {e}\"\n",
    "            print(error_message)\n",
    "            with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                log_file.write(error_message + \"\\n\")\n",
    "                # traceback.print_exc(file=log_file)\n",
    "\n",
    "import argparse\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "\n",
    "async def main(env_path: str = \"../envs/1.env\") -> None:\n",
    "    print(load_dotenv(env_path))\n",
    "    print(f\"Loaded envs from {env_path}\")\n",
    "\n",
    "    with open(os.environ[\"LOG_FILE_PATH\"], \"w\") as log_file:\n",
    "        log_file.write(\"\")\n",
    "\n",
    "    missed_df = pd.DataFrame()\n",
    "    \n",
    "    # Create or overwrite the missed entries file with an empty CSV\n",
    "    missed_df_path = os.environ[\"MISSED_ENTRIES_PATH\"]\n",
    "    missed_df.to_csv(missed_df_path, index=False)\n",
    "    \n",
    "    df = pd.read_csv(os.environ[\"EMAIL_LIST_PATH\"])\n",
    "    df = df[['ORGANIZATION','INDUSTRY', 'WEBSITE', 'COUNTRY', 'CITY', 'FIRSTNAME', 'LASTNAME', 'EMAIL', 'LINKEDIN', 'DESIGNATION']]\n",
    "    df['ORGANIZATION_COPY'] = df['ORGANIZATION']\n",
    "    df = df.set_index('ORGANIZATION_COPY')\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    grouped = (\n",
    "        df.groupby(df.index)\n",
    "        .agg(lambda x: x.dropna().iloc[0] if x.notna().any() else None)\n",
    "    )\n",
    "    i = 1\n",
    "    successful_entries = 0\n",
    "    for organization, group in grouped.iterrows():\n",
    "        if organization == \"A & A Financial Solutions\":\n",
    "            continue\n",
    "        log_message = f\"Processing Entry {i}/{len(grouped)} Organization: {organization}\"\n",
    "        print(log_message)\n",
    "        with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "            log_file.write(log_message + \"\\n\")\n",
    "        row_dict = group.to_dict()\n",
    "        prompt = load_prompt(\"generate_a_search_term\")\n",
    "        prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", prompt),\n",
    "            (\"human\", f\"Generate only one suitable google search term to fetch the official website of the company/service provider/orginization: {row_dict['ORGANIZATION']}\")\n",
    "        ])\n",
    "\n",
    "        result = call_llm(prompt_template = prompt_template, input_dict = {\"row_dict\": row_dict}, structured_output = SearchTermOutput, temperature=0, prompt_name=\"generate_a_search_term\")\n",
    "\n",
    "        try:\n",
    "            print(f\"Processing search term: {result.search_term}\")\n",
    "            links = search_serper(result.search_term)\n",
    "            # print(f\"######### Found {len(links)} links ###########\")\n",
    "            # print(links)\n",
    "            \n",
    "            # Log search results\n",
    "            _write_log(\"search_results.txt\", json.dumps(links, ensure_ascii=False, indent=2))\n",
    "            \n",
    "            most_relevant_result = check_search_relevance(result.search_term, links).most_relevant_result\n",
    "            if most_relevant_result:\n",
    "                print(most_relevant_result)\n",
    "                await process_relevant_links(most_relevant_result, row_dict)\n",
    "                successful_entries += 1\n",
    "                log_message = f\"Successful processed entries {successful_entries}/{len(grouped)} Organization: {organization}\"\n",
    "                print(log_message)\n",
    "                with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                    log_file.write(log_message + \"\\n\")\n",
    "            else:\n",
    "                missed_df = pd.concat([missed_df, group.to_frame().T], ignore_index=True)\n",
    "                missed_df_path = os.environ[\"MISSED_ENTRIES_PATH\"]\n",
    "                missed_df.to_csv(missed_df_path, index=False)\n",
    "                log_message = f\"No relevant link found for search term: {result.search_term}\"\n",
    "                print(log_message)\n",
    "                with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                    log_file.write(log_message + \"\\n\")\n",
    "        except Exception as e:\n",
    "            missed_df = pd.concat([missed_df, group.to_frame().T], ignore_index=True)\n",
    "            missed_df_path = os.environ[\"MISSED_ENTRIES_PATH\"]\n",
    "            missed_df.to_csv(missed_df_path, index=False)\n",
    "            error_message = f\"Error processing Entry {i}/{len(grouped)} Organization: {organization}: {e}\"\n",
    "            print(error_message)\n",
    "            with open(os.environ[\"LOG_FILE_PATH\"], \"a\") as log_file:\n",
    "                log_file.write(error_message + \"\\n\")\n",
    "        i += 1\n",
    "        # if i == 10:\n",
    "        #     break\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description=\"Run the crawl with an .env file.\")\n",
    "\n",
    "#     parser.add_argument(\n",
    "#         \"env_path\",\n",
    "#         help=\"Path to the .env file (e.g., ../envs/1.env).\"\n",
    "#     )\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     asyncio.run(main(args.env_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484ce905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Loaded envs from ../envs/1.env\n",
      "Processing Entry 1/632 Organization: A & M Alansari Auditing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14846/1807723459.py:553: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  response_content = json.dumps(result.dict() if hasattr(result, 'dict') else result.__dict__, ensure_ascii=False, indent=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing search term: A & M Alansari Auditing Dubai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14846/1807723459.py:553: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  response_content = json.dumps(result.dict() if hasattr(result, 'dict') else result.__dict__, ensure_ascii=False, indent=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explanation='am-alansari.com is the official website of A&M Al Ansari Auditing firm in Dubai' link='https://am-alansari.com/'\n",
      "Processing https://am-alansari.com/\n",
      "----------BASE PAGE----------\n",
      "https://am-alansari.com/\n",
      "clean_html: 35663 tokens\n",
      "clean_html: 35663 tokens\n",
      "approved\n",
      "HTML truncated to 15893 chars (~3973 tokens) for fast testing\n",
      "LLM took 62.79s. Produced 5 slices.\n",
      "----------NAV LINKS----------\n",
      "[('ABOUT US', 'https://am-alansari.com/index.php/about-us/', 'about'), ('Our Team', 'https://am-alansari.com/index.php/about-us/#our-team-link', 'team'), ('Financial & Management Reporting', 'https://am-alansari.com/index.php/financial-management-reporting/', 'team'), ('Accounting Software Solutions', 'https://am-alansari.com/index.php/accounting-software-solutions/', 'services'), ('Business Management & Consultation', 'https://am-alansari.com/index.php/business-management-consultation/', 'team'), ('VAT/TAX', 'https://am-alansari.com/index.php/vat-services/', 'services'), ('CAREERS', 'https://am-alansari.com/index.php/careers/', 'careers'), ('CONTACT US', 'https://am-alansari.com/index.php/contact-us/', 'contact'), ('Top 6  benefits of choosing an Accounting Service in Dubai', 'https://am-alansari.com/index.php/2019/04/21/seven-weeks-working-pro-bono-with-a-national-charity/', 'services'), ('5 Things You Most Likely Didnâ€™t Know About Auditing', 'https://am-alansari.com/index.php/2016/01/22/within-the-construction-industry-as-their-overdraft/', 'about')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14846/1807723459.py:553: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  response_content = json.dumps(result.dict() if hasattr(result, 'dict') else result.__dict__, ensure_ascii=False, indent=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "['https://am-alansari.com/index.php/about-us/', 'https://am-alansari.com/index.php/about-us/#our-team-link', 'https://am-alansari.com/index.php/contact-us/', 'https://am-alansari.com/index.php/financial-management-reporting/', 'https://am-alansari.com/index.php/accounting-software-solutions/', 'https://am-alansari.com/index.php/business-management-consultation/', 'https://am-alansari.com/index.php/vat-services/']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74a4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[json_only] parsed slices: [(1, 3), (5, 7)]\n",
      "Line 1: content\n",
      "Line 2: content\n",
      "Line 3: content\n",
      "\n",
      "Line 5: content\n",
      "Line 6: content\n",
      "Line 7: content\n",
      "----------------------------------------\n",
      "[with_fence_and_text] parsed slices: [(2, 4)]\n",
      "Line 2: content\n",
      "Line 3: content\n",
      "Line 4: content\n",
      "----------------------------------------\n",
      "[list_format] parsed slices: [(0, 0), (9, 9)]\n",
      "Line 0: content\n",
      "\n",
      "Line 9: content\n",
      "----------------------------------------\n",
      "[empty] parsed slices: []\n",
      "\n",
      "----------------------------------------\n",
      "[garbage_then_json] parsed slices: [(0, 1)]\n",
      "Line 0: content\n",
      "Line 1: content\n",
      "----------------------------------------\n",
      "LLM took 16.03s. Produced 1 slices.\n",
      "Live slices: [(0, 2)]\n",
      "<h1>Acme Co.</h1>\n",
      "<p>About: We make rockets.</p>\n",
      "<p>Contact: +1-555-1234, info@acme.test</p>\n"
     ]
    }
   ],
   "source": [
    "# # Parser sanity tests (offline)\n",
    "\n",
    "# sample_html_lines = [f\"Line {i}: content\" for i in range(10)]\n",
    "# sample_html = \"\\n\".join(sample_html_lines)\n",
    "\n",
    "# cases = {\n",
    "#     \"json_only\": '{\"slices\": [{\"first_line\":1,\"last_line\":3},{\"start\":5,\"end\":7}] }',\n",
    "#     \"with_fence_and_text\": \"Here are your slices:\\n```json\\n{\\\"slices\\\": [{\\\"first\\\":2, \\\"last\\\":4}]}\\n```\\nThanks.\",\n",
    "#     \"list_format\": '{\"slices\": [[0,0],[9,9]]}',\n",
    "#     \"empty\": '{\"slices\": []}',\n",
    "#     \"garbage_then_json\": 'bla bla {\"slices\": [{\"first_line\":0, \"last_line\":1}]} end'\n",
    "# }\n",
    "\n",
    "# for name, text in cases.items():\n",
    "#     args = _extract_json_from_text(text)\n",
    "#     ss = _build_slice_set_from_args(args)\n",
    "#     print(f\"[{name}] parsed slices:\", [(s.first_line, s.last_line) for s in ss.slices])\n",
    "#     print(get_slices(sample_html, ss))\n",
    "#     print(\"-\"*40)\n",
    "\n",
    "# # Optional live test of generate_slices_simple (disabled by default)\n",
    "# try:\n",
    "#     RUN_LIVE = True\n",
    "#     if RUN_LIVE:\n",
    "#         html = \"\\n\".join([\n",
    "#             \"<h1>Acme Co.</h1>\",\n",
    "#             \"<p>About: We make rockets.</p>\",\n",
    "#             \"<p>Contact: +1-555-1234, info@acme.test</p>\",\n",
    "#         ])\n",
    "#         ss = generate_slices_simple(html)\n",
    "#         print(\"Live slices:\", [(s.first_line, s.last_line) for s in ss.slices])\n",
    "#         print(get_slices(html, ss))\n",
    "# except Exception as e:\n",
    "#     print(\"Live test skipped/failed:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "growbal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
